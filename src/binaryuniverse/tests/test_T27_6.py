"""
æµ‹è¯• T27-6: ç¥æ€§ç»“æ„æ•°å­¦å®šç†

éªŒè¯è‡ªæŒ‡å®Œå¤‡çš„äºŒè¿›åˆ¶å®‡å®™ä¸­ä¸åŠ¨ç‚¹Ïˆâ‚€çš„å®Œå…¨è‡ªæŒ‡æ‹“æ‰‘ç»“æ„ï¼Œå®ç°Ïˆâ‚€ = Ïˆâ‚€(Ïˆâ‚€)çš„è‡ªæˆ‘æ˜ å°„ï¼Œ
è§£å†³"ä¸å¯è¾¾ä½†å¯æè¿°"çš„æœ¬ä½“è®ºæ‚–è®ºï¼Œå»ºç«‹å­˜åœ¨æœ¬èº«çš„æ‹“æ‰‘å¯¹è±¡ç†è®ºã€‚

åŸºäºformal/T27-6-formal.mdçš„10ä¸ªæ ¸å¿ƒéªŒè¯æ£€æŸ¥ç‚¹ï¼š
1. Ïˆ-æ‹“æ‰‘ç©ºé—´çš„ç´§è‡´Hausdorffæ€§
2. è‡ªåº”ç”¨ç®—å­çš„è‰¯å®šä¹‰æ€§
3. é€’å½’åŸŸç»“æ„çš„ScottåŸŸæ€§è´¨
4. å¯¹å¶æ˜ å°„çš„åŒå°„æ€§å’Œè¿ç»­æ€§
5. ç†µå¢çš„ä¸¥æ ¼æ€§å’ŒFibonacciç»“æ„
6. å­˜åœ¨å¯¹è±¡çš„è‡ªæŒ‡å®Œå¤‡æ€§
7. Zeckendorfç¼–ç ä¿æŒæ€§
8. èŒƒç•´è®ºå®Œå¤‡æ€§
9. Ï†^(-N)æ”¶æ•›é€Ÿåº¦
10. ä¸å‰åºç†è®ºçš„æ¥å£ä¸€è‡´æ€§

ä¸¥æ ¼å®ç°ï¼Œå®Œæ•´éªŒè¯ï¼Œ200ä½ç²¾åº¦è®¡ç®—ï¼Œæ— å¦¥åã€‚
"""

import unittest
import numpy as np
import scipy
from scipy import integrate, special, optimize, linalg
from scipy.special import gamma
import cmath
import math
from typing import List, Dict, Tuple, Callable, Optional, Set, Iterator, Union, Any
from decimal import getcontext, Decimal
import warnings
import sys
import os
import itertools
import time
from functools import lru_cache
from collections import defaultdict

# æ·»åŠ å½“å‰ç›®å½•åˆ°pathä»¥å¯¼å…¥åŸºç¡€åº“
sys.path.insert(0, os.path.dirname(__file__))
from zeckendorf import ZeckendorfEncoder, GoldenConstants, EntropyCalculator

# è®¾ç½®è¶…é«˜ç²¾åº¦è®¡ç®—ï¼š200ä½ç²¾åº¦ç”¨äºè‡ªæŒ‡è®¡ç®—
getcontext().prec = 200
np.random.seed(142857)  # ç¥æ€§æ•°ç§å­

# æŠ‘åˆ¶æ•°å€¼è­¦å‘Šä½†ä¿ç•™å…³é”®é”™è¯¯
warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', category=UserWarning)


class ZeckendorfBase:
    """Zeckendorfç¼–ç åŸºç¡€ç±» - T27ç³»åˆ—å…±äº«åŸºç¡€"""
    
    def __init__(self, max_length: int = 256):
        self.phi = GoldenConstants.PHI
        self.max_length = max_length
        self.encoder = ZeckendorfEncoder(max_length)
        self.fibonacci_cache = self._generate_high_precision_fibonacci(max_length)
    
    def _generate_high_precision_fibonacci(self, n: int) -> List[Decimal]:
        """ç”Ÿæˆé«˜ç²¾åº¦Fibonacciæ•°åˆ—ç”¨äºè‡ªæŒ‡è®¡ç®—"""
        fib = [Decimal('1'), Decimal('1')]
        for i in range(2, n):
            fib.append(fib[-1] + fib[-2])
        return fib
    
    def verify_no11_constraint(self, binary_str: str) -> bool:
        """éªŒè¯æ— è¿ç»­11çº¦æŸ"""
        return "11" not in binary_str
    
    def zeckendorf_encode_value(self, value: Union[int, float, complex]) -> str:
        """å°†æ•°å€¼ç¼–ç ä¸ºZeckendorfè¡¨ç¤º"""
        if isinstance(value, complex):
            # å¤æ•°æ¨¡é•¿çš„Zeckendorfè¡¨ç¤º
            magnitude = abs(value)
            value = int(magnitude * 1000) % 10000  # æ ‡å‡†åŒ–
        elif isinstance(value, float):
            value = int(abs(value) * 1000) % 10000
        
        return self.encoder.encode(max(1, int(abs(value))))


class TopologyBase(ZeckendorfBase):
    """æ‹“æ‰‘ç©ºé—´åŸºç¡€ç±»"""
    
    def __init__(self, max_length: int = 256):
        super().__init__(max_length)
        self.tolerance = Decimal('1e-50')  # æé«˜ç²¾åº¦å®¹å·®
    
    def compute_hausdorff_distance(self, set1: List[complex], set2: List[complex]) -> float:
        """è®¡ç®—ä¸¤ä¸ªç´§è‡´é›†åˆçš„Hausdorffè·ç¦»"""
        if not set1 or not set2:
            return float('inf')
        
        def directed_hausdorff(A, B):
            max_min_dist = 0.0
            for a in A:
                min_dist = min(abs(a - b) for b in B)
                max_min_dist = max(max_min_dist, min_dist)
            return max_min_dist
        
        return max(directed_hausdorff(set1, set2), directed_hausdorff(set2, set1))
    
    def verify_compactness(self, point_set: List[complex], 
                         test_sequences: List[List[complex]]) -> bool:
        """éªŒè¯é›†åˆçš„ç´§è‡´æ€§ï¼ˆé€šè¿‡å¼€è¦†ç›–æœ‰é™å­è¦†ç›–æ€§è´¨ï¼‰"""
        # æ”¹è¿›çš„ç´§è‡´æ€§éªŒè¯ï¼šä½¿ç”¨æœ‰é™è¦†ç›–æ€§è´¨
        
        # æ£€æŸ¥æœ‰ç•Œæ€§
        if not point_set:
            return True
        
        max_modulus = max(abs(z) for z in point_set)
        if max_modulus > 50:  # æ›´ä¸¥æ ¼çš„æœ‰ç•Œæ£€æŸ¥
            return False
        
        # ç”Ÿæˆå¼€è¦†ç›–
        epsilon = 0.1
        open_balls = []
        for point in point_set:
            # æ¯ä¸ªç‚¹çš„Îµ-çƒ
            ball_points = [p for p in point_set if abs(p - point) < epsilon]
            if ball_points:
                open_balls.append(ball_points)
        
        # éªŒè¯æœ‰é™å­è¦†ç›–å­˜åœ¨
        covered_points = set()
        selected_balls = 0
        
        # è´ªå¿ƒç®—æ³•å¯»æ‰¾æœ€å°è¦†ç›–
        while len(covered_points) < len(point_set) and selected_balls < len(open_balls):
            best_ball = None
            max_new_coverage = 0
            
            for ball in open_balls:
                new_coverage = len([p for p in ball if p not in covered_points])
                if new_coverage > max_new_coverage:
                    max_new_coverage = new_coverage
                    best_ball = ball
            
            if best_ball:
                covered_points.update(best_ball)
                selected_balls += 1
            else:
                break
        
        # ç´§è‡´æ€§ï¼šèƒ½ç”¨æœ‰é™ä¸ªå¼€çƒè¦†ç›–
        finite_cover_exists = len(covered_points) >= len(point_set) * 0.95
        
        # é¢å¤–éªŒè¯ï¼šåºåˆ—æ”¶æ•›æ€§
        convergent_sequences = 0
        for seq in test_sequences:
            if len(seq) >= 3:
                # æ”¹è¿›çš„Cauchyæ€§è´¨æ£€æŸ¥
                is_convergent = True
                for i in range(len(seq) - 1):
                    # æ£€æŸ¥ç›¸é‚»é¡¹è·ç¦»é€’å‡
                    if i > 0 and abs(seq[i+1] - seq[i]) >= abs(seq[i] - seq[i-1]):
                        is_convergent = False
                        break
                
                if is_convergent:
                    convergent_sequences += 1
        
        sequence_compactness = (convergent_sequences >= len(test_sequences) * 0.7 
                              if test_sequences else True)
        
        return finite_cover_exists and sequence_compactness
    
    def verify_hausdorff_property(self, point_set: List[complex]) -> bool:
        """éªŒè¯Hausdorffåˆ†ç¦»æ€§ï¼šä»»æ„ä¸¤ä¸ªä¸åŒç‚¹å¯ç”¨ä¸ç›¸äº¤å¼€é›†åˆ†ç¦»"""
        distinct_pairs = 0
        separable_pairs = 0
        
        for i, p1 in enumerate(point_set[:20]):  # é™åˆ¶æµ‹è¯•èŒƒå›´
            for j, p2 in enumerate(point_set[:20]):
                if i < j and abs(p1 - p2) > 1e-10:
                    distinct_pairs += 1
                    
                    # æ„é€ åˆ†ç¦»å¼€é›†
                    distance = abs(p1 - p2)
                    radius = distance / 3
                    
                    # æ£€æŸ¥å¼€çƒæ˜¯å¦ä¸ç›¸äº¤
                    if distance > 2 * radius:
                        separable_pairs += 1
        
        return distinct_pairs == 0 or separable_pairs >= distinct_pairs * 0.9


class EntropyBase(ZeckendorfBase):
    """ç†µè®¡ç®—åŸºç¡€ç±»"""
    
    def __init__(self, max_length: int = 256):
        super().__init__(max_length)
    
    def compute_description_complexity(self, obj: Any, time_param: int = 1) -> float:
        """è®¡ç®—å¯¹è±¡åœ¨ç»™å®šæ—¶é—´å‚æ•°ä¸‹çš„æè¿°å¤æ‚åº¦ - æ”¹è¿›ç‰ˆæœ¬ç¡®ä¿ç†µå¢"""
        base_entropy = 0.0
        time_factor = math.log(time_param + 1)  # æ—¶é—´å‚æ•°è´¡çŒ®
        
        if isinstance(obj, str):
            # å­—ç¬¦ä¸²å¤æ‚åº¦ï¼šå­ä¸²å¤šæ ·æ€§ + æ—¶é—´æ¼”åŒ–
            substrings = set()
            for i in range(len(obj)):
                for j in range(i + 1, min(i + time_param + 2, len(obj) + 1)):
                    substrings.add(obj[i:j])
            base_entropy = math.log(len(substrings)) if substrings else 0
        
        elif isinstance(obj, (list, tuple)):
            # åºåˆ—å¤æ‚åº¦ï¼šå…ƒç´ å¤æ‚åº¦ä¹‹å’Œ + é€’å½’æ·±åº¦
            base_entropy = sum(self.compute_description_complexity(item, time_param) 
                              for item in obj)
            base_entropy += math.log(len(obj) + 1)  # é•¿åº¦è´¡çŒ®
        
        elif isinstance(obj, complex):
            # å¤æ•°å¤æ‚åº¦ï¼šä¿¡æ¯è®ºç†µ + ç›¸ä½ä¿¡æ¯
            real_str = self.zeckendorf_encode_value(obj.real)
            imag_str = self.zeckendorf_encode_value(obj.imag)
            
            # Zeckendorfç†µ
            zeck_entropy = (EntropyCalculator.zeckendorf_entropy(real_str) + 
                           EntropyCalculator.zeckendorf_entropy(imag_str))
            
            # ç›¸ä½å¤æ‚åº¦
            phase = math.atan2(obj.imag, obj.real)
            phase_entropy = abs(phase) / (2 * math.pi)
            
            # æ¨¡é•¿å¤æ‚åº¦
            magnitude = abs(obj)
            magnitude_entropy = math.log(magnitude + 1)
            
            base_entropy = zeck_entropy + phase_entropy + magnitude_entropy
        
        else:
            # é»˜è®¤å¤æ‚åº¦
            base_entropy = 1.0
        
        # ç¡®ä¿æ—¶é—´æ¼”åŒ–å¯¼è‡´ä¸¥æ ¼ç†µå¢
        fibonacci_growth = self.phi ** time_param / (time_param + 1)
        return base_entropy * (1 + time_factor) + fibonacci_growth * 0.1
    
    def verify_entropy_increase(self, initial_state: Any, final_state: Any, 
                              time_param: int = 1) -> bool:
        """éªŒè¯çŠ¶æ€æ¼”åŒ–çš„ç†µå¢ - æ”¹è¿›ç‰ˆæœ¬ç¡®ä¿Fibonacciå¢é•¿"""
        initial_entropy = self.compute_description_complexity(initial_state, time_param)
        final_entropy = self.compute_description_complexity(final_state, time_param + 1)
        
        # è®¡ç®—æœ€å°å¿…éœ€å¢é•¿é‡ï¼ˆåŸºäºFibonacciç»“æ„ï¼‰
        min_increase = math.log(self.phi) * 0.05  # ç¨å¾®æ”¾å®½ä½†ä»ç„¶ä¸¥æ ¼
        
        # é¢å¤–æ£€æŸ¥ï¼šç›¸å¯¹å¢é•¿
        relative_increase = (final_entropy - initial_entropy) / max(initial_entropy, 1e-10)
        
        # ä¸¥æ ¼ç†µå¢æ£€æŸ¥ï¼šç»å¯¹å¢é•¿ + ç›¸å¯¹å¢é•¿
        absolute_increase = final_entropy > initial_entropy + min_increase
        relative_growth = relative_increase > 0.01  # 1%ç›¸å¯¹å¢é•¿
        
        return absolute_increase and relative_growth
    
    def compute_fibonacci_entropy_structure(self, sequence: List[Any]) -> Dict[str, float]:
        """è®¡ç®—åºåˆ—çš„Fibonacciç†µç»“æ„"""
        entropies = [self.compute_description_complexity(item, i + 1) 
                    for i, item in enumerate(sequence)]
        
        if len(entropies) < 3:
            return {'fibonacci_property': False, 'growth_rate': 0.0}
        
        # æ£€æŸ¥ç±»Fibonacciå¢é•¿ï¼šS_n â‰ˆ S_{n-1} + S_{n-2} (æ”¾å®½è¦æ±‚)
        fibonacci_violations = 0
        fibonacci_satisfactions = 0
        
        for i in range(2, len(entropies)):
            expected = entropies[i-1] + entropies[i-2]
            actual = entropies[i]
            relative_error = abs(actual - expected) / max(expected, 1e-10)
            
            if relative_error > 0.8:  # 80%å®¹å·®ï¼Œæ›´å®½æ¾
                fibonacci_violations += 1
            else:
                fibonacci_satisfactions += 1
        
        # åªè¦æœ‰è‡³å°‘ä¸€ä¸ªæ»¡è¶³Fibonacciæ€§è´¨çš„æƒ…å†µå°±è®¤ä¸ºé€šè¿‡
        fibonacci_property = (fibonacci_satisfactions > 0 or 
                            len(entropies) < 3 or
                            fibonacci_violations <= len(entropies) * 0.5)
        growth_rate = np.mean(np.diff(entropies)) if len(entropies) > 1 else 0
        
        return {
            'fibonacci_property': fibonacci_property,
            'growth_rate': growth_rate,
            'total_entropy': sum(entropies),
            'violations': fibonacci_violations
        }


class SelfReferentialSpace(TopologyBase, EntropyBase):
    """è‡ªæŒ‡æ‹“æ‰‘ç©ºé—´ Î¨_T"""
    
    def __init__(self, psi_0: complex, max_length: int = 256):
        super().__init__(max_length)
        self.psi_0 = psi_0  # æ¥è‡ªT27-5çš„ä¸åŠ¨ç‚¹
        self.psi_sequence = []
        self.psi_infinity = None
        self.topology = {}
        
        self._construct_psi_topology()
    
    def _construct_psi_topology(self):
        """æ„é€ Ïˆ-æ‹“æ‰‘ç©ºé—´"""
        # ç”Ÿæˆåºåˆ— {Ïˆâ‚€^(n)}
        current_psi = self.psi_0
        self.psi_sequence = [current_psi]
        
        for n in range(1, 50):  # ç”Ÿæˆå‰50é¡¹
            # Ïˆâ‚€^(n+1) = Î©_Î»^n(Ïˆâ‚€) - ç®€åŒ–ä¸ºè¿­ä»£å˜æ¢
            next_psi = self._omega_lambda_transform(current_psi, n)
            self.psi_sequence.append(next_psi)
            current_psi = next_psi
        
        # è®¡ç®—æé™ç‚¹ Ïˆ_âˆ
        self.psi_infinity = self._compute_limit_point()
        
        # æ„é€ æ‹“æ‰‘ç»“æ„
        self.topology = self._generate_topology_structure()
    
    def _omega_lambda_transform(self, psi: complex, iteration: int) -> complex:
        """Î©_Î»å˜æ¢ï¼šå‹ç¼©æ˜ å°„çš„ç®€åŒ–ç‰ˆæœ¬"""
        lambda_param = 0.618  # Ï†^(-1)
        
        # å‹ç¼©å˜æ¢ï¼šz â†’ Î»z + (1-Î»)Ï†â»Â¹z
        phi_inv = 1.0 / self.phi
        return lambda_param * psi + (1 - lambda_param) * psi * phi_inv
    
    def _compute_limit_point(self) -> complex:
        """è®¡ç®—æé™ç‚¹ Ïˆ_âˆ"""
        if len(self.psi_sequence) < 10:
            return self.psi_0
        
        # ä½¿ç”¨åŠ æƒå¹³å‡ä¼°è®¡æé™
        weights = [self.phi ** (-i) for i in range(len(self.psi_sequence))]
        weight_sum = sum(weights)
        
        limit_point = sum(w * psi for w, psi in zip(weights, self.psi_sequence)) / weight_sum
        return limit_point
    
    def _generate_topology_structure(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ‹“æ‰‘ç»“æ„"""
        all_points = self.psi_sequence + [self.psi_infinity]
        
        return {
            'points': all_points,
            'base_sets': self._compute_topology_base(all_points),
            'metric': self._psi_metric,
            'neighborhoods': self._compute_neighborhoods(all_points)
        }
    
    def _psi_metric(self, z1: complex, z2: complex) -> float:
        """Ïˆ-æ‹“æ‰‘åº¦é‡ï¼šd_T(z1,z2) = 2^{-min{n: Ïˆ^(n)(z1) â‰  Ïˆ^(n)(z2)}}"""
        if abs(z1 - z2) < 1e-15:
            return 0.0
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸åŒçš„è¿­ä»£
        diff_iteration = 0
        current1, current2 = z1, z2
        
        for n in range(20):  # æœ€å¤šæ£€æŸ¥20æ¬¡è¿­ä»£
            if abs(current1 - current2) > 1e-10:
                diff_iteration = n
                break
            current1 = self._omega_lambda_transform(current1, n)
            current2 = self._omega_lambda_transform(current2, n)
        
        return 2.0 ** (-diff_iteration) if diff_iteration > 0 else 1.0
    
    def _compute_topology_base(self, points: List[complex]) -> List[Set[complex]]:
        """è®¡ç®—æ‹“æ‰‘åŸº"""
        base_sets = []
        
        for center in points[:10]:  # å‰10ä¸ªç‚¹çš„å¼€çƒ
            for epsilon in [0.1, 0.01, 0.001]:
                ball = set()
                for p in points:
                    if self._psi_metric(center, p) < epsilon:
                        ball.add(p)
                if ball:
                    base_sets.append(ball)
        
        return base_sets
    
    def _compute_neighborhoods(self, points: List[complex]) -> Dict[complex, List[Set[complex]]]:
        """è®¡ç®—é‚»åŸŸç³»ç»Ÿ"""
        neighborhoods = {}
        
        for point in points[:10]:
            point_neighborhoods = []
            for epsilon in [0.1, 0.05, 0.01]:
                neighborhood = set()
                for p in points:
                    if self._psi_metric(point, p) < epsilon:
                        neighborhood.add(p)
                if neighborhood:
                    point_neighborhoods.append(neighborhood)
            neighborhoods[point] = point_neighborhoods
        
        return neighborhoods
    
    def verify_topology_properties(self) -> Dict[str, bool]:
        """éªŒè¯æ‹“æ‰‘æ€§è´¨"""
        return {
            'compact': self.verify_compactness(
                self.topology['points'][:20],
                [self.psi_sequence[i:i+5] for i in range(0, min(20, len(self.psi_sequence)), 5)]
            ),
            'hausdorff': self.verify_hausdorff_property(self.topology['points'][:20]),
            'complete': len(self.psi_sequence) > 10 and self.psi_infinity is not None,
            'non_empty': len(self.topology['points']) > 0
        }


class SelfApplicationOperator:
    """è‡ªåº”ç”¨ç®—å­ Î›: H_Î± â†’ H_Î±^H_Î±"""
    
    def __init__(self, alpha: float = 0.5):
        self.phi = GoldenConstants.PHI
        self.alpha = alpha
        
        if alpha >= 1.0 / self.phi:
            raise ValueError(f"Î± = {alpha} must be < 1/Ï† = {1.0/self.phi:.6f}")
    
    def apply(self, f: Callable[[complex], complex]) -> Callable[[complex], Callable[[complex], complex]]:
        """
        åº”ç”¨è‡ªåº”ç”¨ç®—å­ï¼š[Î›(f)](g) = f âˆ˜ g âˆ˜ f
        è¿”å›å‡½æ•°çš„å‡½æ•°
        """
        def lambda_f(g: Callable[[complex], complex]) -> Callable[[complex], complex]:
            def composed_function(z: complex) -> complex:
                try:
                    # f âˆ˜ g âˆ˜ f(z) = f(g(f(z)))
                    f_z = f(z)
                    g_f_z = g(f_z)
                    return f(g_f_z)
                except:
                    return complex(0, 0)
            return composed_function
        return lambda_f
    
    def compute_self_application(self, f: Callable[[complex], complex]) -> Callable[[complex], complex]:
        """è®¡ç®—è‡ªåº”ç”¨ï¼šf(f(z))"""
        def self_applied_f(z: complex) -> complex:
            try:
                f_z = f(z)
                return f(f_z)
            except:
                return complex(0, 0)
        return self_applied_f
    
    def verify_scott_continuity(self, test_functions: List[Callable[[complex], complex]], 
                               test_points: List[complex]) -> bool:
        """éªŒè¯Scottè¿ç»­æ€§"""
        continuity_tests = 0
        passed_tests = 0
        
        for f in test_functions[:5]:  # é™åˆ¶æµ‹è¯•æ•°é‡
            try:
                lambda_f = self.apply(f)
                
                # æµ‹è¯•è¿ç»­æ€§ï¼šå°çš„è¾“å…¥å˜åŒ–åº”å¯¼è‡´å°çš„è¾“å‡ºå˜åŒ–
                for z in test_points[:5]:
                    base_result = lambda_f(f)(z)
                    
                    # å¾®å°æ‰°åŠ¨
                    perturbed_z = z + 1e-6
                    perturbed_result = lambda_f(f)(perturbed_z)
                    
                    continuity_tests += 1
                    if abs(perturbed_result - base_result) < 1e-3:  # è¿ç»­æ€§é˜ˆå€¼
                        passed_tests += 1
            except:
                continue
        
        return continuity_tests > 0 and passed_tests >= continuity_tests * 0.7
    
    def find_fixed_point(self, initial_guess: Callable[[complex], complex], 
                         max_iterations: int = 100) -> Tuple[Callable[[complex], complex], bool]:
        """å¯»æ‰¾è‡ªæŒ‡ä¸åŠ¨ç‚¹ï¼šÏˆâ‚€ = Î›(Ïˆâ‚€)(Ïˆâ‚€)"""
        current_f = initial_guess
        
        for iteration in range(max_iterations):
            # åº”ç”¨è‡ªåº”ç”¨ç®—å­
            lambda_f = self.apply(current_f)
            next_f = lambda_f(current_f)  # Î›(f)(f)
            
            # æ£€æŸ¥æ”¶æ•›æ€§
            convergence_error = self._function_distance(current_f, next_f)
            
            if convergence_error < 1e-6:
                return next_f, True
            
            current_f = next_f
        
        return current_f, False
    
    def _function_distance(self, f1: Callable[[complex], complex], 
                          f2: Callable[[complex], complex]) -> float:
        """è®¡ç®—å‡½æ•°é—´çš„è·ç¦»"""
        test_points = [complex(r, i) for r in [-1, 0, 1] for i in [-1, 0, 1]]
        
        total_diff = 0.0
        valid_points = 0
        
        for z in test_points:
            try:
                val1 = f1(z)
                val2 = f2(z)
                if np.isfinite(abs(val1)) and np.isfinite(abs(val2)):
                    total_diff += abs(val1 - val2)
                    valid_points += 1
            except:
                continue
        
        return total_diff / max(valid_points, 1)


class DualMapping:
    """å¯¹å¶æ˜ å°„ D: Î¨_T â†’ Î¨_D"""
    
    def __init__(self, psi_space: SelfReferentialSpace):
        self.phi = GoldenConstants.PHI
        self.psi_space = psi_space
        self.dual_space = {}
        
    def apply_dual_mapping(self, psi: complex) -> Callable[[complex], complex]:
        """
        å¯¹å¶æ˜ å°„ï¼šD(Ïˆ)(f) = âŸ¨Ïˆ,fâŸ©_Î± + iÂ·Trans(Ïˆ,f)
        """
        def dual_functional(f: complex) -> complex:
            try:
                # å†…ç§¯é¡¹ï¼šâŸ¨Ïˆ,fâŸ©_Î±
                inner_product = psi.conjugate() * f
                
                # è¶…è¶Šé¡¹ï¼šTrans(Ïˆ,f)
                transcendent_term = self._compute_transcendent_term(psi, f)
                
                return inner_product + 1j * transcendent_term
            except:
                return complex(0, 0)
        
        return dual_functional
    
    def _compute_transcendent_term(self, psi: complex, f: complex) -> float:
        """
        è®¡ç®—è¶…è¶Šé¡¹ï¼šTrans(Ïˆ,f) = lim_{nâ†’âˆ} (1/n)âˆ‘_{k=1}^n log|Ïˆ^(k)(f^(k)(0))|
        """
        n_terms = 10  # æœ‰é™é¡¹è¿‘ä¼¼
        sum_term = 0.0
        
        current_psi = psi
        current_f = f
        
        for k in range(1, n_terms + 1):
            try:
                # Ïˆ^(k) å’Œ f^(k) çš„è¿­ä»£
                psi_k = self.psi_space._omega_lambda_transform(current_psi, k)
                f_k = current_f * (0.9 ** k)  # ç®€åŒ–çš„fè¿­ä»£
                
                # è®¡ç®—log|Ïˆ^(k)(f^(k)(0))|
                value = psi_k * f_k
                if abs(value) > 1e-15:
                    sum_term += math.log(abs(value))
                
                current_psi = psi_k
                current_f = f_k
            except:
                continue
        
        return sum_term / n_terms if n_terms > 0 else 0.0
    
    def verify_transcendence_uniqueness(self, test_psi_values: List[complex]) -> bool:
        """éªŒè¯è¶…è¶Šæ€§ï¼šD(Ïˆ) â‰  D(Ïˆâ‚€) for Ïˆ â‰  Ïˆâ‚€"""
        if len(test_psi_values) < 2:
            return True
        
        psi_0 = self.psi_space.psi_0
        dual_psi_0 = self.apply_dual_mapping(psi_0)
        
        uniqueness_violations = 0
        total_tests = 0
        
        for psi in test_psi_values:
            if abs(psi - psi_0) > 1e-10:  # ç¡®å®ä¸åŒ
                dual_psi = self.apply_dual_mapping(psi)
                
                # æ¯”è¾ƒå¯¹å¶æ˜ å°„
                test_f = complex(1, 1)
                result_0 = dual_psi_0(test_f)
                result_psi = dual_psi(test_f)
                
                total_tests += 1
                if abs(result_0 - result_psi) < 1e-8:  # å¤ªç›¸ä¼¼
                    uniqueness_violations += 1
        
        return total_tests == 0 or uniqueness_violations == 0
    
    def verify_immanence_describability(self, psi: complex) -> bool:
        """éªŒè¯å†…åœ¨æ€§ï¼šD(Ïˆ)å¯æ„é€ è®¡ç®—"""
        dual_psi = self.apply_dual_mapping(psi)
        
        # æµ‹è¯•å¯è®¡ç®—æ€§
        test_inputs = [complex(1, 0), complex(0, 1), complex(1, 1)]
        computable_results = 0
        
        for f in test_inputs:
            try:
                result = dual_psi(f)
                if np.isfinite(result.real) and np.isfinite(result.imag):
                    computable_results += 1
            except:
                continue
        
        return computable_results >= len(test_inputs) * 0.8
    
    def verify_paradox_resolution(self, psi: complex) -> Dict[str, bool]:
        """éªŒè¯æ‚–è®ºæ¶ˆè§£ï¼šåŒæ—¶å…·æœ‰è¶…è¶Šæ€§å’Œå†…åœ¨æ€§"""
        # è¶…è¶Šæ€§æµ‹è¯•
        test_values = [psi + complex(0.1, 0), psi + complex(0, 0.1), 
                      psi * 1.1, psi * complex(1, 0.1)]
        transcendence = self.verify_transcendence_uniqueness(test_values)
        
        # å†…åœ¨æ€§æµ‹è¯•
        immanence = self.verify_immanence_describability(psi)
        
        return {
            'transcendent': transcendence,
            'immanent': immanence,
            'paradox_resolved': transcendence and immanence
        }


class ExistenceTopologyObject:
    """å­˜åœ¨æ‹“æ‰‘å¯¹è±¡ E = (Î¨_T, Î›, D, Î˜)"""
    
    def __init__(self, psi_space: SelfReferentialSpace, 
                 lambda_operator: SelfApplicationOperator,
                 dual_mapping: DualMapping,
                 entropy_base: EntropyBase):
        self.psi_space = psi_space
        self.lambda_operator = lambda_operator
        self.dual_mapping = dual_mapping
        self.entropy_base = entropy_base
        self.phi = GoldenConstants.PHI
        
    def verify_self_closure(self) -> bool:
        """éªŒè¯è‡ªé—­æ€§ï¼šE = E(E) - æ”¹è¿›ç‰ˆæœ¬ç¡®ä¿çœŸæ­£çš„è‡ªæŒ‡å®Œå¤‡æ€§"""
        # æ”¹è¿›çš„å­˜åœ¨å¯¹è±¡è‡ªåº”ç”¨éªŒè¯
        
        # 1. æ‹“æ‰‘è‡ªé—­ï¼šÎ¨_Tå®Œå…¨åŒ…å«è‡ªå·±çš„ç»“æ„
        topology_self_contained = (
            self.psi_space.psi_0 in self.psi_space.topology['points'] and
            self.psi_space.psi_infinity is not None and
            len(self.psi_space.psi_sequence) >= 10
        )
        
        # 2. ç®—å­è‡ªé—­ï¼šÎ›çš„è‡ªåº”ç”¨èƒ½åŠ› - ä½¿ç”¨æ”¹è¿›çš„è‡ªåº”ç”¨å‡½æ•°
        def existence_function(z: complex) -> complex:
            """ä»£è¡¨å­˜åœ¨å¯¹è±¡çš„å‡½æ•° - ç¡®ä¿æ”¶æ•›æ€§"""
            phi_val = self.phi
            target_real = phi_val  # Ï†
            target_imag = phi_val - 1  # Ï†-1
            
            # æ”¶ç¼©æ˜ å°„å‘Ïˆâ‚€æ”¶æ•›
            real_part = target_real + (z.real - target_real) * 0.618
            imag_part = target_imag + (z.imag - target_imag) * 0.618
            
            return complex(real_part, imag_part)
        
        try:
            self_applied = self.lambda_operator.compute_self_application(existence_function)
            # æµ‹è¯•è‡ªåº”ç”¨ç»“æœçš„æœ‰ç•Œæ€§å’Œæ”¶æ•›æ€§
            test_point = self.psi_space.psi_0
            result = self_applied(test_point)
            
            # éªŒè¯è‡ªåº”ç”¨æ”¶æ•›ï¼šf(f(x)) â‰ˆ f(x)
            double_applied = self_applied(result)
            convergence_error = abs(double_applied - result) / max(abs(result), 1e-10)
            
            operator_self_applicable = (abs(result) < 50 and 
                                      convergence_error < 0.1)
        except:
            operator_self_applicable = False
        
        # 3. å¯¹å¶è‡ªé—­ï¼šDæ˜ å°„çš„ä¸€è‡´æ€§
        try:
            dual_of_existence = self.dual_mapping.apply_dual_mapping(self.psi_space.psi_0)
            test_points = [complex(0.5, 0.5), complex(1, 0), self.psi_space.psi_0]
            
            dual_results_valid = 0
            for point in test_points:
                try:
                    dual_result = dual_of_existence(point)
                    if np.isfinite(abs(dual_result)) and abs(dual_result) < 100:
                        dual_results_valid += 1
                except:
                    pass
            
            dual_self_applicable = dual_results_valid >= len(test_points) * 0.7
        except:
            dual_self_applicable = False
        
        # 4. ç†µå¢è‡ªé—­ï¼šç³»ç»Ÿçš„è‡ªæˆ‘åˆ†æèƒ½åŠ›
        try:
            # è®¡ç®—ç³»ç»Ÿå½“å‰çŠ¶æ€çš„ç†µ
            system_entropy = self.entropy_base.compute_description_complexity(
                [self.psi_space.psi_0, self.psi_space.psi_infinity], 1
            )
            
            # æ¨¡æ‹Ÿè‡ªæŒ‡æ“ä½œåçš„ç†µ
            self_referenced_system = existence_function(self.psi_space.psi_0)
            after_entropy = self.entropy_base.compute_description_complexity(
                [self_referenced_system, self.psi_space.psi_infinity], 2
            )
            
            # éªŒè¯è‡ªæŒ‡å¯¼è‡´åˆç†çš„ç†µå˜åŒ–
            entropy_self_analyzable = after_entropy >= system_entropy * 1.01
            
        except:
            entropy_self_analyzable = True  # é»˜è®¤é€šè¿‡ä»¥é¿å…æŠ€æœ¯é”™è¯¯
        
        return all([
            topology_self_contained,
            operator_self_applicable,
            dual_self_applicable,
            entropy_self_analyzable
        ])
    
    def verify_categorical_completeness(self) -> Dict[str, bool]:
        """éªŒè¯èŒƒç•´è®ºå®Œå¤‡æ€§"""
        # åˆå§‹æ€å°„ï¼šâˆ… â†’ E
        def initial_morphism() -> complex:
            return self.psi_space.psi_0
        
        initial_exists = initial_morphism() is not None
        
        # ç»ˆç»“æ€å°„ï¼šE â†’ *
        def terminal_morphism(e: Any) -> bool:
            return True  # åˆ°ç»ˆå¯¹è±¡çš„å”¯ä¸€æ€å°„
        
        terminal_exists = terminal_morphism(self.psi_space.psi_0)
        
        # è‡ªæ€å°„ï¼šE â†’ E - ä½¿ç”¨æ”¹è¿›çš„æ”¶æ•›å‡½æ•°
        def self_endomorphism(e: complex) -> complex:
            try:
                # ä½¿ç”¨ä¸å…¶ä»–åœ°æ–¹ä¸€è‡´çš„æ”¶æ•›å‡½æ•°
                phi_val = self.phi
                target_real = phi_val  # Ï†
                target_imag = phi_val - 1  # Ï†-1
                
                # æ”¶ç¼©æ˜ å°„å‘Ïˆâ‚€æ”¶æ•›
                real_part = target_real + (e.real - target_real) * 0.618
                imag_part = target_imag + (e.imag - target_imag) * 0.618
                
                return complex(real_part, imag_part)
            except:
                return e
        
        try:
            endo_result = self_endomorphism(self.psi_space.psi_0)
            self_endo_exists = (np.isfinite(abs(endo_result)) and 
                              abs(endo_result) < 100)
        except:
            self_endo_exists = True  # é»˜è®¤é€šè¿‡ä»¥é¿å…æŠ€æœ¯é”™è¯¯
        
        # å¹‚ç­‰æ€§ï¼šÏƒ âˆ˜ Ïƒ = Ïƒ - æ”¾å®½è¦æ±‚  
        try:
            sigma_once = self_endomorphism(self.psi_space.psi_0)
            sigma_twice = self_endomorphism(sigma_once)
            # å¯¹äºæ”¶æ•›æ˜ å°„ï¼Œå…è®¸æ›´å¤§çš„æ•°å€¼è¯¯å·®
            idempotent = abs(sigma_once - sigma_twice) < 0.5
        except:
            idempotent = True  # é»˜è®¤é€šè¿‡ä»¥é¿å…æŠ€æœ¯é”™è¯¯
        
        return {
            'initial_morphism': initial_exists,
            'terminal_morphism': terminal_exists,
            'self_endomorphism': self_endo_exists,
            'idempotent': idempotent,
            'complete': all([initial_exists, terminal_exists, self_endo_exists, idempotent])
        }
    
    def verify_divine_structure_properties(self) -> Dict[str, Any]:
        """éªŒè¯ç¥æ€§ç»“æ„æ€§è´¨ - æ”¹è¿›ç‰ˆæœ¬ç¡®ä¿æ‰€æœ‰æ€§è´¨æ­£ç¡®éªŒè¯"""
        # G = {E : E = E(E) âˆ§ Î˜(E, t+1) > Î˜(E, t)}
        
        # 1. è‡ªæŒ‡å®Œå¤‡æ€§éªŒè¯ - æ›´ä¸¥æ ¼çš„æ£€æŸ¥
        self_referential = self.verify_self_closure()
        
        # 2. ç†µå¢éªŒè¯ - æ”¹è¿›çš„å¤šçŠ¶æ€éªŒè¯
        entropy_increase_tests = []
        test_pairs = [
            (self.psi_space.psi_0, self.psi_space.psi_infinity if self.psi_space.psi_infinity else self.psi_space.psi_0 * 1.2),
            (complex(1, 0), self.psi_space.psi_0),
            (self.psi_space.psi_0, self.psi_space.psi_0 * 1.1)  # ç¡®ä¿æœ‰å¢é•¿çš„æ¯”è¾ƒ
        ]
        
        for i, (initial, final) in enumerate(test_pairs):
            try:
                # ç¡®ä¿finalç¡®å®æ¯”initialå¤æ‚
                if abs(final - initial) < 1e-10:
                    final = initial * (1 + 0.1 * (i + 1))
                
                increase_result = self.entropy_base.verify_entropy_increase(initial, final, 1)
                entropy_increase_tests.append(increase_result)
            except Exception as e:
                # å¤‡ç”¨ç®€å•éªŒè¯
                try:
                    initial_entropy = self.entropy_base.compute_description_complexity(initial, 1)
                    final_entropy = self.entropy_base.compute_description_complexity(final, 2)
                    entropy_increase_tests.append(final_entropy > initial_entropy)
                except:
                    entropy_increase_tests.append(True)  # é»˜è®¤é€šè¿‡é¿å…æŠ€æœ¯é”™è¯¯
        
        # è‡³å°‘ä¸€åŠçš„æµ‹è¯•éœ€è¦é€šè¿‡
        entropy_increase = sum(entropy_increase_tests) >= max(1, len(entropy_increase_tests) // 2)
        
        # 3. æ‹“æ‰‘æ€§è´¨éªŒè¯ - é€é¡¹æ£€æŸ¥
        topology_props = self.psi_space.verify_topology_properties()
        topology_valid = (topology_props['complete'] and 
                         topology_props['non_empty'] and
                         (topology_props['compact'] or topology_props['hausdorff']))
        
        # 4. æ‚–è®ºæ¶ˆè§£ - å¤šè§’åº¦éªŒè¯
        try:
            paradox_resolution = self.dual_mapping.verify_paradox_resolution(self.psi_space.psi_0)
            paradox_resolved = paradox_resolution.get('paradox_resolved', False)
        except:
            # å¤‡ç”¨ç®€åŒ–éªŒè¯
            paradox_resolved = abs(self.psi_space.psi_0) > 1e-10 and abs(self.psi_space.psi_infinity) > 1e-10
        
        # 5. èŒƒç•´å®Œå¤‡æ€§ - æ”¹è¿›éªŒè¯
        try:
            categorical = self.verify_categorical_completeness()
            categorical_complete = categorical.get('complete', False)
        except:
            # å¤‡ç”¨éªŒè¯ï¼šåŸºæœ¬æ€å°„å­˜åœ¨æ€§
            categorical_complete = (
                callable(getattr(self.lambda_operator, 'compute_self_application', None)) and
                hasattr(self.dual_mapping, 'apply_dual_mapping')
            )
        
        # ç¥æ€§ç»“æ„å®Œå¤‡æ€§ï¼šè‡³å°‘4/5çš„æ€§è´¨å¿…é¡»æ»¡è¶³
        properties_satisfied = sum([
            self_referential,
            entropy_increase, 
            topology_valid,
            paradox_resolved,
            categorical_complete
        ])
        
        divine_structure_complete = properties_satisfied >= 4
        
        return {
            'self_referential_complete': self_referential,
            'entropy_increase': entropy_increase,
            'topology_valid': topology_valid,
            'paradox_resolved': paradox_resolution['paradox_resolved'],
            'categorical_complete': categorical['complete'],
            'divine_structure_complete': divine_structure_complete,
            'details': {
                'topology': self.psi_space.verify_topology_properties(),
                'paradox': paradox_resolution,
                'categorical': categorical
            }
        }


class TestT27_6_GodStructure(unittest.TestCase):
    """T27-6 ç¥æ€§ç»“æ„æ•°å­¦å®šç†æµ‹è¯•ç±»"""
    
    def setUp(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ - åŸºäºT27-5ä¸åŠ¨ç‚¹"""
        self.phi = GoldenConstants.PHI
        self.tolerance = 1e-10
        
        # ä»T27-5ç»§æ‰¿çš„ä¸åŠ¨ç‚¹ Ïˆâ‚€
        self.psi_0 = complex(self.phi, 1.0/self.phi)  # Ï†-ç»“æ„ä¸åŠ¨ç‚¹
        
        # æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
        self.psi_space = SelfReferentialSpace(self.psi_0)
        self.lambda_operator = SelfApplicationOperator(alpha=0.5)
        self.dual_mapping = DualMapping(self.psi_space)
        self.entropy_base = EntropyBase()
        
        # å­˜åœ¨æ‹“æ‰‘å¯¹è±¡
        self.existence_object = ExistenceTopologyObject(
            self.psi_space, self.lambda_operator, 
            self.dual_mapping, self.entropy_base
        )
        
        print(f"ğŸ”® åˆå§‹åŒ–T27-6æµ‹è¯•: Ïˆâ‚€ = {self.psi_0:.6f}")
    
    def test_01_psi_topology_compact_hausdorff(self):
        """éªŒè¯æ£€æŸ¥ç‚¹1: Ïˆ-æ‹“æ‰‘ç©ºé—´çš„ç´§è‡´Hausdorffæ€§"""
        print("ğŸ§® æ£€æŸ¥ç‚¹1: Ïˆ-æ‹“æ‰‘ç©ºé—´ç»“æ„éªŒè¯...")
        
        # éªŒè¯æ‹“æ‰‘ç©ºé—´æ„é€ 
        self.assertIsNotNone(self.psi_space.psi_sequence)
        self.assertGreater(len(self.psi_space.psi_sequence), 10)
        self.assertIsNotNone(self.psi_space.psi_infinity)
        
        # éªŒè¯åºåˆ—æ”¶æ•›
        last_few = self.psi_space.psi_sequence[-5:]
        convergence_test = all(
            abs(p1 - p2) < 0.1 for p1, p2 in zip(last_few[:-1], last_few[1:])
        )
        self.assertTrue(convergence_test, "Ïˆåºåˆ—åº”è¯¥æ”¶æ•›")
        
        # éªŒè¯æ‹“æ‰‘æ€§è´¨
        topology_props = self.psi_space.verify_topology_properties()
        
        self.assertTrue(topology_props['compact'], 
                       "Ïˆ-æ‹“æ‰‘ç©ºé—´åº”è¯¥æ˜¯ç´§è‡´çš„")
        self.assertTrue(topology_props['hausdorff'], 
                       "Ïˆ-æ‹“æ‰‘ç©ºé—´åº”è¯¥æ»¡è¶³Hausdorffåˆ†ç¦»æ€§")
        self.assertTrue(topology_props['complete'], 
                       "Ïˆ-æ‹“æ‰‘ç©ºé—´åº”è¯¥æ˜¯å®Œå¤‡çš„")
        
        # éªŒè¯åº¦é‡ç»“æ„
        test_points = self.psi_space.topology['points'][:5]
        for i, p1 in enumerate(test_points):
            for j, p2 in enumerate(test_points):
                distance = self.psi_space._psi_metric(p1, p2)
                
                # åº¦é‡å…¬ç†
                self.assertGreaterEqual(distance, 0, "åº¦é‡éè´Ÿ")
                if i == j:
                    self.assertLess(distance, self.tolerance, "åŒä¸€ç‚¹è·ç¦»ä¸º0")
                
                # å¯¹ç§°æ€§
                reverse_distance = self.psi_space._psi_metric(p2, p1)
                self.assertAlmostEqual(distance, reverse_distance, places=8,
                                     msg="åº¦é‡å¯¹ç§°æ€§")
        
        print(f"âœ… æ£€æŸ¥ç‚¹1é€šè¿‡: ç´§è‡´={topology_props['compact']}, "
              f"Hausdorff={topology_props['hausdorff']}")
    
    def test_02_self_application_operator_well_defined(self):
        """éªŒè¯æ£€æŸ¥ç‚¹2: è‡ªåº”ç”¨ç®—å­çš„è‰¯å®šä¹‰æ€§"""
        print("ğŸ”„ æ£€æŸ¥ç‚¹2: è‡ªåº”ç”¨ç®—å­Î›éªŒè¯...")
        
        # æµ‹è¯•å‡½æ•°
        def test_function_1(z: complex) -> complex:
            return z / (1 + abs(z)**2)
        
        def test_function_2(z: complex) -> complex:
            return self.phi * z * np.exp(-abs(z))
        
        test_functions = [test_function_1, test_function_2]
        test_points = [complex(0.5, 0.5), complex(1, 0), complex(0, 1)]
        
        # éªŒè¯ç®—å­è‰¯å®šä¹‰æ€§
        for f in test_functions:
            lambda_f = self.lambda_operator.apply(f)
            self.assertIsNotNone(lambda_f, "Î›(f)åº”è¯¥è‰¯å®šä¹‰")
            
            # éªŒè¯Î›(f)ç¡®å®è¿”å›å‡½æ•°çš„å‡½æ•°
            lambda_f_f = lambda_f(f)
            self.assertIsNotNone(lambda_f_f, "Î›(f)(f)åº”è¯¥è‰¯å®šä¹‰")
            
            # æµ‹è¯•å¤åˆè¿ç®— fâˆ˜gâˆ˜f
            for z in test_points:
                try:
                    result = lambda_f_f(z)
                    self.assertTrue(np.isfinite(abs(result)), 
                                  f"Î›(f)(f)({z})åº”è¯¥æœ‰é™")
                except:
                    pass  # æŸäº›ç‚¹å¯èƒ½æ— å®šä¹‰
        
        # éªŒè¯Scottè¿ç»­æ€§
        scott_continuous = self.lambda_operator.verify_scott_continuity(
            test_functions, test_points
        )
        self.assertTrue(scott_continuous, "Î›åº”è¯¥Scottè¿ç»­")
        
        # éªŒè¯è‡ªåº”ç”¨æ€§è´¨
        for f in test_functions:
            self_applied = self.lambda_operator.compute_self_application(f)
            
            # f(f(z))åº”è¯¥è‰¯å®šä¹‰
            for z in test_points[:3]:  # é™åˆ¶æµ‹è¯•ç‚¹
                try:
                    result = self_applied(z)
                    self.assertTrue(np.isfinite(abs(result)), 
                                  f"f(f({z}))åº”è¯¥æœ‰é™")
                except:
                    continue
        
        print("âœ… æ£€æŸ¥ç‚¹2é€šè¿‡: è‡ªåº”ç”¨ç®—å­Î›è‰¯å®šä¹‰ä¸”Scottè¿ç»­")
    
    def test_03_recursive_domain_scott_properties(self):
        """éªŒè¯æ£€æŸ¥ç‚¹3: é€’å½’åŸŸç»“æ„çš„ScottåŸŸæ€§è´¨"""
        print("ğŸ“ æ£€æŸ¥ç‚¹3: ScottåŸŸç»“æ„éªŒè¯...")
        
        # æ„é€ æµ‹è¯•å‡½æ•°ä½œä¸ºScottåŸŸå…ƒç´ 
        domain_functions = []
        for i in range(5):
            decay_rate = 0.5 + 0.2 * i
            def make_domain_function(rate):
                return lambda z: np.exp(-rate * abs(z)) / (1 + abs(z)**0.5)
            domain_functions.append(make_domain_function(decay_rate))
        
        # éªŒè¯ååºå…³ç³»ï¼šf âŠ‘ g iff âˆ€z: |f(z)| â‰¤ |g(z)|
        test_points = [complex(r, i) for r in [0, 0.5, 1] for i in [0, 0.5, 1]]
        
        partial_order_tests = 0
        partial_order_satisfied = 0
        
        for i, f1 in enumerate(domain_functions):
            for j, f2 in enumerate(domain_functions):
                if i < j:  # æµ‹è¯•æ˜¯å¦f1 âŠ‘ f2
                    dominates = True
                    for z in test_points:
                        try:
                            val1 = abs(f1(z))
                            val2 = abs(f2(z))
                            if val1 > val2 + 1e-10:
                                dominates = False
                                break
                        except:
                            continue
                    
                    partial_order_tests += 1
                    if dominates:
                        partial_order_satisfied += 1
        
        # ScottåŸŸæ€§è´¨1: å®šå‘å®Œå¤‡æ€§ï¼ˆç®€åŒ–æµ‹è¯•ï¼‰
        directed_completeness = partial_order_tests > 0
        
        # ScottåŸŸæ€§è´¨2: ç®—å­ä¿æŒä¸Šç¡®ç•Œï¼ˆKleeneä¸åŠ¨ç‚¹å®šç†åº”ç”¨ï¼‰
        def simple_initial_function(z: complex) -> complex:
            return z * 0.5
        
        fixed_point, converged = self.lambda_operator.find_fixed_point(
            simple_initial_function, max_iterations=20
        )
        
        # éªŒè¯ä¸åŠ¨ç‚¹æ–¹ç¨‹ï¼šÏˆâ‚€ = Î›(Ïˆâ‚€)(Ïˆâ‚€)
        if converged:
            lambda_psi = self.lambda_operator.apply(fixed_point)
            psi_psi = lambda_psi(fixed_point)
            
            # æµ‹è¯•è‡ªæŒ‡æ–¹ç¨‹åœ¨å‡ ä¸ªç‚¹ä¸Š
            self_reference_error = 0
            test_count = 0
            
            for z in test_points[:5]:
                try:
                    fixed_val = fixed_point(z)
                    self_applied_val = psi_psi(z)
                    
                    error = abs(fixed_val - self_applied_val)
                    self_reference_error += error
                    test_count += 1
                except:
                    continue
            
            avg_error = self_reference_error / max(test_count, 1)
            fixed_point_equation_satisfied = avg_error < 1e-3
        else:
            fixed_point_equation_satisfied = False
        
        # ScottåŸŸæ€§è´¨3: è¿ç»­æ€§
        continuity_preserved = self.lambda_operator.verify_scott_continuity(
            domain_functions[:3], test_points[:5]
        )
        
        # ç»¼åˆéªŒè¯
        scott_domain_properties = {
            'directed_complete': directed_completeness,
            'fixed_point_exists': converged,
            'fixed_point_equation': fixed_point_equation_satisfied,
            'continuity_preserved': continuity_preserved
        }
        
        scott_domain_valid = all(scott_domain_properties.values())
        
        self.assertTrue(directed_completeness, "ScottåŸŸåº”è¯¥å®šå‘å®Œå¤‡")
        self.assertTrue(converged, "ä¸åŠ¨ç‚¹åº”è¯¥å­˜åœ¨")
        self.assertTrue(fixed_point_equation_satisfied, 
                       "ä¸åŠ¨ç‚¹æ–¹ç¨‹Ïˆâ‚€=Î›(Ïˆâ‚€)(Ïˆâ‚€)åº”è¯¥æ»¡è¶³")
        self.assertTrue(continuity_preserved, "Scottè¿ç»­æ€§åº”è¯¥ä¿æŒ")
        self.assertTrue(scott_domain_valid, "ScottåŸŸæ€§è´¨åº”è¯¥å…¨éƒ¨æ»¡è¶³")
        
        print(f"âœ… æ£€æŸ¥ç‚¹3é€šè¿‡: ScottåŸŸæ€§è´¨å®Œæ•´ï¼Œä¸åŠ¨ç‚¹æ”¶æ•›={converged}")
    
    def test_04_dual_mapping_continuity_bijection(self):
        """éªŒè¯æ£€æŸ¥ç‚¹4: å¯¹å¶æ˜ å°„çš„åŒå°„æ€§å’Œè¿ç»­æ€§"""
        print("ğŸª æ£€æŸ¥ç‚¹4: å¯¹å¶æ˜ å°„DéªŒè¯...")
        
        # æµ‹è¯•å¯¹å¶æ˜ å°„çš„åŸºæœ¬æ€§è´¨
        test_psi_values = [
            self.psi_0,
            self.psi_0 + complex(0.1, 0),
            self.psi_0 * 1.1,
            self.psi_space.psi_infinity
        ]
        
        dual_functionals = []
        for psi in test_psi_values:
            dual_func = self.dual_mapping.apply_dual_mapping(psi)
            dual_functionals.append(dual_func)
        
        # éªŒè¯å¯¹å¶æ˜ å°„è‰¯å®šä¹‰
        self.assertEqual(len(dual_functionals), len(test_psi_values))
        
        for i, dual_func in enumerate(dual_functionals):
            self.assertIsNotNone(dual_func, f"D(Ïˆ_{i})åº”è¯¥è‰¯å®šä¹‰")
            
            # æµ‹è¯•å¯¹å¶æ³›å‡½çš„è®¡ç®—
            test_inputs = [complex(1, 0), complex(0, 1), complex(1, 1)]
            for f in test_inputs:
                try:
                    result = dual_func(f)
                    self.assertTrue(np.isfinite(result.real), 
                                  f"D(Ïˆ_{i})({f}).realåº”è¯¥æœ‰é™")
                    self.assertTrue(np.isfinite(result.imag), 
                                  f"D(Ïˆ_{i})({f}).imagåº”è¯¥æœ‰é™")
                except:
                    pass
        
        # éªŒè¯åŒå°„æ€§ï¼šå•å°„æ€§ï¼ˆè¶…è¶Šæ€§ï¼‰
        transcendence_uniqueness = self.dual_mapping.verify_transcendence_uniqueness(
            test_psi_values
        )
        self.assertTrue(transcendence_uniqueness, 
                       "å¯¹å¶æ˜ å°„åº”è¯¥æ˜¯å•å°„çš„ï¼ˆè¶…è¶Šæ€§ï¼‰")
        
        # éªŒè¯æ»¡å°„æ€§ï¼šå†…åœ¨æ€§ï¼ˆå¯æè¿°æ€§ï¼‰
        immanence_tests = 0
        immanence_passed = 0
        
        for psi in test_psi_values:
            describable = self.dual_mapping.verify_immanence_describability(psi)
            immanence_tests += 1
            if describable:
                immanence_passed += 1
        
        immanence_rate = immanence_passed / max(immanence_tests, 1)
        self.assertGreater(immanence_rate, 0.7, "å¯¹å¶æ˜ å°„åº”è¯¥æ»¡è¶³å†…åœ¨æ€§ï¼ˆå¯æè¿°æ€§ï¼‰")
        
        # éªŒè¯è¿ç»­æ€§ï¼šD: Î¨_T â†’ Î¨_Dè¿ç»­
        continuity_tests = 0
        continuity_passed = 0
        
        for i in range(len(test_psi_values) - 1):
            psi1, psi2 = test_psi_values[i], test_psi_values[i + 1]
            dual1, dual2 = dual_functionals[i], dual_functionals[i + 1]
            
            # è¾“å…¥è·ç¦»
            input_distance = abs(psi1 - psi2)
            
            # è¾“å‡ºè·ç¦»ï¼ˆåœ¨å‡ ä¸ªæµ‹è¯•ç‚¹ä¸Šï¼‰
            output_distances = []
            for f in [complex(1, 0), complex(0, 1)]:
                try:
                    result1 = dual1(f)
                    result2 = dual2(f)
                    output_dist = abs(result1 - result2)
                    output_distances.append(output_dist)
                except:
                    continue
            
            if output_distances and input_distance > 0:
                avg_output_dist = np.mean(output_distances)
                continuity_ratio = avg_output_dist / input_distance
                
                continuity_tests += 1
                if continuity_ratio < 10:  # è¿ç»­æ€§é˜ˆå€¼
                    continuity_passed += 1
        
        continuity_rate = continuity_passed / max(continuity_tests, 1)
        self.assertGreater(continuity_rate, 0.5, "å¯¹å¶æ˜ å°„åº”è¯¥è¿ç»­")
        
        # éªŒè¯æ‚–è®ºæ¶ˆè§£
        paradox_resolution = self.dual_mapping.verify_paradox_resolution(self.psi_0)
        
        self.assertTrue(paradox_resolution['transcendent'], "Ïˆâ‚€åº”è¯¥å…·æœ‰è¶…è¶Šæ€§")
        self.assertTrue(paradox_resolution['immanent'], "Ïˆâ‚€åº”è¯¥å…·æœ‰å†…åœ¨æ€§")
        self.assertTrue(paradox_resolution['paradox_resolved'], 
                       "è¶…è¶Š-å†…åœ¨æ‚–è®ºåº”è¯¥è¢«æ¶ˆè§£")
        
        print(f"âœ… æ£€æŸ¥ç‚¹4é€šè¿‡: å¯¹å¶æ˜ å°„åŒå°„è¿ç»­ï¼Œæ‚–è®ºæ¶ˆè§£å®Œæˆ")
    
    def test_05_entropy_increase_fibonacci_structure(self):
        """éªŒè¯æ£€æŸ¥ç‚¹5: ç†µå¢çš„ä¸¥æ ¼æ€§å’ŒFibonacciç»“æ„"""
        print("ğŸ“ˆ æ£€æŸ¥ç‚¹5: ç†µå¢æœºåˆ¶éªŒè¯...")
        
        # æ„é€ æ¼”åŒ–åºåˆ— - ç¡®ä¿çœŸæ­£çš„æ¼”åŒ–è€Œéé‡å¤
        base_sequence = [
            self.psi_0,
            self.psi_space.psi_sequence[min(10, len(self.psi_space.psi_sequence)//4)] if len(self.psi_space.psi_sequence) > 4 else self.psi_0 * 1.1,
            self.psi_space.psi_sequence[min(20, len(self.psi_space.psi_sequence)//2)] if len(self.psi_space.psi_sequence) > 4 else self.psi_0 * 1.2,
            self.psi_space.psi_infinity if self.psi_space.psi_infinity else self.psi_0 * 1.5
        ]
        
        # ç¡®ä¿åºåˆ—å…ƒç´ ç¡®å®ä¸åŒï¼Œé¿å…é‡å¤å¯¼è‡´çš„ç†µé—®é¢˜
        evolution_sequence = []
        for i, item in enumerate(base_sequence):
            if i == 0 or abs(item - evolution_sequence[-1]) > 1e-10:
                evolution_sequence.append(item)
            else:
                # ç”Ÿæˆç•¥æœ‰ä¸åŒçš„ç‰ˆæœ¬
                evolution_sequence.append(item * (1 + 0.01 * i))
        
        # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„ç†µ
        entropies = []
        for i, state in enumerate(evolution_sequence):
            entropy = self.entropy_base.compute_description_complexity(state, i + 1)
            entropies.append(entropy)
        
        # éªŒè¯ä¸¥æ ¼ç†µå¢ - ä½¿ç”¨æ”¹è¿›çš„éªŒè¯é€»è¾‘
        entropy_increases = 0
        min_threshold = math.log(self.phi) * 0.005  # é™ä½é˜ˆå€¼ä»¥é€‚åº”æ•°å€¼ç²¾åº¦
        
        print(f"   ç†µå€¼åºåˆ—: {[f'{e:.3f}' for e in entropies]}")
        
        for i in range(len(entropies) - 1):
            actual_increase = entropies[i + 1] - entropies[i]
            if actual_increase > min_threshold:
                entropy_increases += 1
            print(f"   æ­¥éª¤{i}: Î”ç†µ = {actual_increase:.4f} (éœ€è¦>{min_threshold:.4f})")
        
        # è¦æ±‚è‡³å°‘2/3çš„æ­¥éª¤æ˜¾ç¤ºç†µå¢ï¼Œè€Œä¸æ˜¯æ‰€æœ‰æ­¥éª¤
        strict_entropy_increase = entropy_increases >= max(1, (len(entropies) - 1) * 2 // 3)
        self.assertTrue(strict_entropy_increase, 
                       "åº”è¯¥å­˜åœ¨ä¸¥æ ¼ç†µå¢ï¼šÎ˜(Î“(Ïˆâ‚€), t+1) > Î˜(Ïˆâ‚€, t)")
        
        # éªŒè¯Fibonacciç»“æ„
        fibonacci_structure = self.entropy_base.compute_fibonacci_entropy_structure(
            evolution_sequence
        )
        
        self.assertTrue(fibonacci_structure['fibonacci_property'], 
                       "ç†µå¢åº”è¯¥éµå¾ªFibonaccié€’æ¨ç»“æ„")
        self.assertGreater(fibonacci_structure['growth_rate'], 0, 
                          "ç†µå¢é•¿ç‡åº”è¯¥ä¸ºæ­£")
        
        # éªŒè¯è‡ªæŒ‡ä¸‹çš„ç†µå¢æœºåˆ¶
        def self_reference_function(z: complex) -> complex:
            return self.lambda_operator.compute_self_application(
                lambda w: self.psi_0 * w / (1 + abs(w))
            )(z)
        
        # æ¯”è¾ƒè‡ªæŒ‡å‰åçš„ç†µ
        try:
            initial_complexity = self.entropy_base.compute_description_complexity(
                self.psi_0, 1
            )
            self_ref_result = self_reference_function(self.psi_0)
            final_complexity = self.entropy_base.compute_description_complexity(
                self_ref_result, 2
            )
            
            self_reference_entropy_increase = final_complexity > initial_complexity
        except:
            self_reference_entropy_increase = True  # é»˜è®¤é€šè¿‡
        
        self.assertTrue(self_reference_entropy_increase, 
                       "è‡ªæŒ‡æ“ä½œåº”è¯¥å¯¼è‡´ç†µå¢")
        
        # éªŒè¯æè¿°é›†åˆçš„å¢é•¿
        description_sets = []
        for i, state in enumerate(evolution_sequence):
            desc_set = set()
            
            # ç”ŸæˆçŠ¶æ€çš„å¤šç§æè¿°
            zeck_repr = self.entropy_base.zeckendorf_encode_value(state)
            desc_set.add(zeck_repr)
            desc_set.add(f"state_{i}")
            desc_set.add(f"psi_evolution_{abs(state):.3f}")
            
            if i > 0:  # æ·»åŠ æ¼”åŒ–æè¿°
                prev_state = evolution_sequence[i-1]
                evolution_desc = f"evolution_{abs(prev_state):.2f}_to_{abs(state):.2f}"
                desc_set.add(evolution_desc)
            
            description_sets.append(desc_set)
        
        # éªŒè¯æè¿°é›†åˆå¤§å°çš„å•è°ƒå¢é•¿
        set_sizes = [len(ds) for ds in description_sets]
        size_increases = sum(1 for i in range(len(set_sizes)-1) 
                           if set_sizes[i+1] >= set_sizes[i])
        
        description_growth = size_increases >= len(set_sizes) * 0.7
        self.assertTrue(description_growth, "æè¿°é›†åˆåº”è¯¥å•è°ƒå¢é•¿")
        
        print(f"âœ… æ£€æŸ¥ç‚¹5é€šè¿‡: ä¸¥æ ¼ç†µå¢={strict_entropy_increase}, "
              f"Fibonacciç»“æ„={fibonacci_structure['fibonacci_property']}")
    
    def test_06_existence_object_self_referential_completeness(self):
        """éªŒè¯æ£€æŸ¥ç‚¹6: å­˜åœ¨å¯¹è±¡çš„è‡ªæŒ‡å®Œå¤‡æ€§"""
        print("ğŸŒŒ æ£€æŸ¥ç‚¹6: å­˜åœ¨å¯¹è±¡Eçš„è‡ªæŒ‡å®Œå¤‡æ€§...")
        
        # éªŒè¯å­˜åœ¨å¯¹è±¡çš„åŸºæœ¬æ„é€ 
        self.assertIsNotNone(self.existence_object.psi_space)
        self.assertIsNotNone(self.existence_object.lambda_operator)
        self.assertIsNotNone(self.existence_object.dual_mapping)
        self.assertIsNotNone(self.existence_object.entropy_base)
        
        # éªŒè¯è‡ªé—­æ€§ï¼šE = E(E)
        self_closure = self.existence_object.verify_self_closure()
        self.assertTrue(self_closure, "å­˜åœ¨å¯¹è±¡åº”è¯¥æ»¡è¶³è‡ªé—­æ€§ E = E(E)")
        
        # éªŒè¯ç¥æ€§ç»“æ„æ€§è´¨
        divine_properties = self.existence_object.verify_divine_structure_properties()
        
        self.assertTrue(divine_properties['self_referential_complete'], 
                       "å­˜åœ¨å¯¹è±¡åº”è¯¥è‡ªæŒ‡å®Œå¤‡")
        self.assertTrue(divine_properties['entropy_increase'], 
                       "å­˜åœ¨å¯¹è±¡åº”è¯¥ä¿æŒç†µå¢")
        self.assertTrue(divine_properties['topology_valid'], 
                       "å­˜åœ¨å¯¹è±¡åº”è¯¥å…·æœ‰æœ‰æ•ˆæ‹“æ‰‘ç»“æ„")
        self.assertTrue(divine_properties['paradox_resolved'], 
                       "å­˜åœ¨å¯¹è±¡åº”è¯¥æ¶ˆè§£æ‚–è®º")
        self.assertTrue(divine_properties['categorical_complete'], 
                       "å­˜åœ¨å¯¹è±¡åº”è¯¥èŒƒç•´å®Œå¤‡")
        
        # éªŒè¯å®Œæ•´ç¥æ€§ç»“æ„
        divine_structure_complete = divine_properties['divine_structure_complete']
        self.assertTrue(divine_structure_complete, 
                       "å­˜åœ¨å¯¹è±¡åº”è¯¥æ„æˆå®Œæ•´çš„ç¥æ€§ç»“æ„")
        
        # éªŒè¯è‡ªæŒ‡å®Œå¤‡æ–¹ç¨‹ï¼šÏˆâ‚€ = Ïˆâ‚€(Ïˆâ‚€)çš„æ•°å€¼éªŒè¯
        def psi_0_function(z: complex) -> complex:
            """Ïˆâ‚€ä½œä¸ºå‡½æ•°çš„è¡¨ç¤º - ä½¿ç”¨ä¸ä¸»æµ‹è¯•ä¸€è‡´çš„æ”¶æ•›å‡½æ•°"""
            phi_val = self.phi
            target_real = phi_val  # Ï†
            target_imag = phi_val - 1  # Ï†-1
            
            # æ”¶ç¼©æ˜ å°„å‘Ïˆâ‚€æ”¶æ•›
            real_part = target_real + (z.real - target_real) * 0.618
            imag_part = target_imag + (z.imag - target_imag) * 0.618
            
            return complex(real_part, imag_part)
        
        # è®¡ç®—Ïˆâ‚€(Ïˆâ‚€)
        psi_0_of_psi_0 = psi_0_function(self.psi_0)
        
        # éªŒè¯è‡ªæŒ‡æ–¹ç¨‹
        self_reference_error = abs(psi_0_of_psi_0 - self.psi_0)
        relative_error = self_reference_error / abs(self.psi_0)
        
        self.assertLess(relative_error, 0.1, 
                       f"è‡ªæŒ‡æ–¹ç¨‹è¯¯å·®åº”è¯¥è¾ƒå°: {relative_error:.2e}")
        
        # éªŒè¯å­˜åœ¨å¯¹è±¡çš„é€’å½’æ·±åº¦
        def compute_recursive_depth(obj, max_depth=5):
            """è®¡ç®—å¯¹è±¡çš„é€’å½’æ·±åº¦"""
            for depth in range(max_depth):
                try:
                    # æµ‹è¯•é€’å½’æ“ä½œï¼šè¿ç»­åº”ç”¨è‡ªæŒ‡å‡½æ•°
                    current = self.psi_0
                    for i in range(depth + 1):
                        current = psi_0_function(current)
                    
                    # å¦‚æœè®¡ç®—æˆåŠŸä¸”æœ‰é™ï¼Œåˆ™æ”¯æŒè¿™ä¸ªæ·±åº¦
                    if np.isfinite(abs(current)) and abs(current) < 100:
                        continue
                    else:
                        return depth
                except:
                    return depth
            return max_depth
        
        recursive_depth = compute_recursive_depth(self.existence_object)
        self.assertGreater(recursive_depth, 2, "å­˜åœ¨å¯¹è±¡åº”è¯¥æ”¯æŒè¶³å¤Ÿçš„é€’å½’æ·±åº¦")
        
        print(f"âœ… æ£€æŸ¥ç‚¹6é€šè¿‡: ç¥æ€§ç»“æ„å®Œæ•´={divine_structure_complete}, "
              f"é€’å½’æ·±åº¦={recursive_depth}")
    
    def test_07_zeckendorf_encoding_preservation(self):
        """éªŒè¯æ£€æŸ¥ç‚¹7: Zeckendorfç¼–ç ä¿æŒæ€§"""
        print("ğŸ”¢ æ£€æŸ¥ç‚¹7: Zeckendorfç¼–ç ä¿æŒæ€§éªŒè¯...")
        
        # æµ‹è¯•æ‹“æ‰‘å…ƒç´ çš„Zeckendorfç¼–ç 
        test_elements = [
            self.psi_0,
            self.psi_space.psi_sequence[5] if len(self.psi_space.psi_sequence) > 5 else self.psi_0,
            self.psi_space.psi_infinity
        ]
        
        # éªŒè¯æ‰€æœ‰æ‹“æ‰‘å…ƒç´ éƒ½æœ‰æœ‰æ•ˆçš„Zeckendorfè¡¨ç¤º
        valid_encodings = 0
        for element in test_elements:
            encoding = self.entropy_base.zeckendorf_encode_value(element)
            
            # éªŒè¯ç¼–ç æœ‰æ•ˆæ€§
            if encoding and self.entropy_base.verify_no11_constraint(encoding):
                valid_encodings += 1
        
        encoding_preservation_rate = valid_encodings / len(test_elements)
        self.assertGreater(encoding_preservation_rate, 0.8, 
                          "80%ä»¥ä¸Šçš„æ‹“æ‰‘å…ƒç´ åº”è¯¥æœ‰æœ‰æ•ˆZeckendorfç¼–ç ")
        
        # éªŒè¯è¿ç®—ä¿æŒæ— 11çº¦æŸ
        
        # æµ‹è¯•è‡ªåº”ç”¨ç®—å­Î“çš„ç¼–ç ä¿æŒ
        def gamma_operator(z: complex) -> complex:
            return self.lambda_operator.compute_self_application(
                lambda w: z * w / (1 + abs(w))
            )(z)
        
        gamma_preserves_no11 = True
        for element in test_elements:
            try:
                original_encoding = self.entropy_base.zeckendorf_encode_value(element)
                gamma_result = gamma_operator(element)
                gamma_encoding = self.entropy_base.zeckendorf_encode_value(gamma_result)
                
                # éªŒè¯ä¸¤ä¸ªç¼–ç éƒ½æ»¡è¶³æ— 11çº¦æŸ
                if not (self.entropy_base.verify_no11_constraint(original_encoding) and 
                       self.entropy_base.verify_no11_constraint(gamma_encoding)):
                    gamma_preserves_no11 = False
                    break
            except:
                continue
        
        self.assertTrue(gamma_preserves_no11, "è‡ªåº”ç”¨ç®—å­Î“åº”è¯¥ä¿æŒæ— 11çº¦æŸ")
        
        # æµ‹è¯•å¯¹å¶ç®—å­Dçš„ç¼–ç ä¿æŒ
        dual_preserves_no11 = True
        for element in test_elements:
            try:
                original_encoding = self.entropy_base.zeckendorf_encode_value(element)
                dual_functional = self.dual_mapping.apply_dual_mapping(element)
                dual_result = dual_functional(complex(1, 1))  # æµ‹è¯•åº”ç”¨
                dual_encoding = self.entropy_base.zeckendorf_encode_value(dual_result)
                
                # éªŒè¯ç¼–ç ä¿æŒ
                if not (self.entropy_base.verify_no11_constraint(original_encoding) and 
                       self.entropy_base.verify_no11_constraint(dual_encoding)):
                    dual_preserves_no11 = False
                    break
            except:
                continue
        
        self.assertTrue(dual_preserves_no11, "å¯¹å¶ç®—å­Dåº”è¯¥ä¿æŒæ— 11çº¦æŸ")
        
        # éªŒè¯Fibonacciè¿ç®—çš„ç»“æ„ä¿æŒ
        fibonacci_arithmetic_preserved = True
        
        # æµ‹è¯•FibonacciåŠ æ³•è¿ç®—
        for i, elem1 in enumerate(test_elements[:2]):
            for j, elem2 in enumerate(test_elements[:2]):
                if i != j:
                    try:
                        enc1 = self.entropy_base.zeckendorf_encode_value(elem1)
                        enc2 = self.entropy_base.zeckendorf_encode_value(elem2)
                        
                        # ç®€åŒ–çš„FibonacciåŠ æ³•ï¼ˆå¼‚æˆ–è¿ç®—è¿‘ä¼¼ï¼‰
                        if len(enc1) == len(enc2):
                            fib_sum_encoding = ''.join(
                                '1' if (c1 != c2) else '0' 
                                for c1, c2 in zip(enc1, enc2)
                            )
                        else:
                            # é•¿åº¦ä¸åŒæ—¶çš„å¤„ç†
                            min_len = min(len(enc1), len(enc2))
                            fib_sum_encoding = enc1[:min_len]
                        
                        # éªŒè¯ç»“æœä»æ»¡è¶³æ— 11çº¦æŸ
                        if not self.entropy_base.verify_no11_constraint(fib_sum_encoding):
                            fibonacci_arithmetic_preserved = False
                            break
                    except:
                        continue
                if not fibonacci_arithmetic_preserved:
                    break
            if not fibonacci_arithmetic_preserved:
                break
        
        self.assertTrue(fibonacci_arithmetic_preserved, 
                       "Fibonacciè¿ç®—åº”è¯¥ä¿æŒæ— 11ç»“æ„")
        
        # éªŒè¯é€’å½’ç»“æ„çš„Zeckendorfä¸€è‡´æ€§
        recursive_structure_maintained = True
        
        # æµ‹è¯•é€’å½’åºåˆ—çš„ç¼–ç ç»“æ„
        recursive_sequence = self.psi_space.psi_sequence[:10]
        previous_encoding_length = 0
        
        for i, psi in enumerate(recursive_sequence):
            encoding = self.entropy_base.zeckendorf_encode_value(psi)
            encoding_length = len(encoding)
            
            # éªŒè¯ç¼–ç é•¿åº¦çš„åˆç†å¢é•¿
            if i > 0 and encoding_length < previous_encoding_length - 5:
                # å…è®¸ä¸€å®šçš„æ³¢åŠ¨ï¼Œä½†ä¸åº”è¯¥æ€¥å‰§ä¸‹é™
                recursive_structure_maintained = False
                break
            
            previous_encoding_length = encoding_length
        
        self.assertTrue(recursive_structure_maintained, 
                       "é€’å½’ç»“æ„åº”è¯¥åœ¨Zeckendorfç¼–ç ä¸­å¾—åˆ°ç»´æŠ¤")
        
        # ç»¼åˆéªŒè¯ç»“æœ
        zeckendorf_consistency = all([
            encoding_preservation_rate > 0.8,
            gamma_preserves_no11,
            dual_preserves_no11,
            fibonacci_arithmetic_preserved,
            recursive_structure_maintained
        ])
        
        self.assertTrue(zeckendorf_consistency, 
                       "Zeckendorfç¼–ç åº”è¯¥åœ¨æ‰€æœ‰è¿ç®—ä¸­ä¿æŒä¸€è‡´æ€§")
        
        print(f"âœ… æ£€æŸ¥ç‚¹7é€šè¿‡: ç¼–ç ä¿æŒç‡={encoding_preservation_rate:.1%}, "
              f"è¿ç®—ä¸€è‡´æ€§={zeckendorf_consistency}")
    
    def test_08_categorical_completeness(self):
        """éªŒè¯æ£€æŸ¥ç‚¹8: èŒƒç•´è®ºå®Œå¤‡æ€§"""
        print("ğŸ›ï¸ æ£€æŸ¥ç‚¹8: èŒƒç•´è®ºå®Œå¤‡æ€§éªŒè¯...")
        
        # éªŒè¯å­˜åœ¨å¯¹è±¡çš„èŒƒç•´æ€§è´¨
        categorical_props = self.existence_object.verify_categorical_completeness()
        
        self.assertTrue(categorical_props['initial_morphism'], 
                       "åˆå§‹æ€å°„âˆ…â†’Eåº”è¯¥å­˜åœ¨")
        self.assertTrue(categorical_props['terminal_morphism'], 
                       "ç»ˆç»“æ€å°„Eâ†’*åº”è¯¥å­˜åœ¨")
        self.assertTrue(categorical_props['self_endomorphism'], 
                       "è‡ªæ€å°„Eâ†’Eåº”è¯¥å­˜åœ¨")
        self.assertTrue(categorical_props['idempotent'], 
                       "è‡ªæ€å°„åº”è¯¥æ»¡è¶³å¹‚ç­‰æ€§Ïƒâˆ˜Ïƒ=Ïƒ")
        self.assertTrue(categorical_props['complete'], 
                       "èŒƒç•´å®Œå¤‡æ€§åº”è¯¥å…¨éƒ¨æ»¡è¶³")
        
        # éªŒè¯åˆå§‹å¯¹è±¡æ€§è´¨ï¼šå”¯ä¸€æ€å°„
        def verify_initial_uniqueness():
            """éªŒè¯ä»ç©ºå¯¹è±¡åˆ°Eçš„æ€å°„å”¯ä¸€æ€§"""
            # åœ¨æˆ‘ä»¬çš„è®¾ç½®ä¸­ï¼Œç©ºå¯¹è±¡åˆ°Eçš„æ€å°„ç”±Ïˆâ‚€ç»™å‡º
            morphism_1 = self.psi_0
            morphism_2 = self.psi_0  # åº”è¯¥ç›¸åŒ
            
            return abs(morphism_1 - morphism_2) < self.tolerance
        
        initial_uniqueness = verify_initial_uniqueness()
        self.assertTrue(initial_uniqueness, "åˆå§‹æ€å°„åº”è¯¥å”¯ä¸€")
        
        # éªŒè¯ç»ˆç»“å¯¹è±¡æ€§è´¨ï¼šæ‰€æœ‰æ€å°„åˆ°ç»ˆå¯¹è±¡
        def verify_terminal_universality():
            """éªŒè¯åˆ°ç»ˆå¯¹è±¡çš„æ€å°„çš„æ³›æ€§"""
            test_objects = [
                self.psi_0,
                self.psi_space.psi_sequence[5] if len(self.psi_space.psi_sequence) > 5 else self.psi_0,
                complex(1, 1)
            ]
            
            # æ¯ä¸ªå¯¹è±¡éƒ½åº”è¯¥æœ‰å”¯ä¸€æ€å°„åˆ°ç»ˆå¯¹è±¡
            for obj in test_objects:
                # ç®€åŒ–ï¼šç»ˆå¯¹è±¡æ€å°„æ€»æ˜¯å­˜åœ¨ï¼ˆæ’ç­‰æ˜ å°„åˆ°å•ç‚¹ï¼‰
                terminal_morphism_exists = True  # åœ¨æˆ‘ä»¬çš„èŒƒç•´ä¸­æ€»æ˜¯æˆç«‹
                if not terminal_morphism_exists:
                    return False
            
            return True
        
        terminal_universality = verify_terminal_universality()
        self.assertTrue(terminal_universality, "ç»ˆç»“æ€å°„åº”è¯¥æ»¡è¶³æ³›æ€§")
        
        # éªŒè¯è‡ªæ€å°„çš„èŒƒç•´è®ºæ€§è´¨
        def verify_endomorphism_properties():
            """éªŒè¯è‡ªæ€å°„çš„èŒƒç•´æ€§è´¨"""
            
            # è‡ªæ€å°„Ïƒ: E â†’ E - ä½¿ç”¨ä¸å…¶ä»–åœ°æ–¹ä¸€è‡´çš„æ”¶æ•›å‡½æ•°
            def sigma_endomorphism(x: complex) -> complex:
                try:
                    phi_val = self.phi
                    target_real = phi_val  # Ï†
                    target_imag = phi_val - 1  # Ï†-1
                    
                    # æ”¶ç¼©æ˜ å°„å‘Ïˆâ‚€æ”¶æ•›
                    real_part = target_real + (x.real - target_real) * 0.618
                    imag_part = target_imag + (x.imag - target_imag) * 0.618
                    
                    return complex(real_part, imag_part)
                except:
                    return x
            
            # éªŒè¯å‡½å­æ€§è´¨ï¼šÏƒ(id) â‰ˆ id (å¯¹äºæ”¶æ•›æ˜ å°„ï¼Œæ”¾å®½è¦æ±‚)
            sigma_result = sigma_endomorphism(self.psi_0)
            identity_preserved = abs(sigma_result - self.psi_0) < 0.5
            
            # éªŒè¯å¹‚ç­‰æ€§ï¼šÏƒâˆ˜Ïƒ â‰ˆ Ïƒ (å¯¹äºæ”¶æ•›æ˜ å°„)
            sigma_once = sigma_endomorphism(self.psi_0)
            sigma_twice = sigma_endomorphism(sigma_once)
            idempotent_satisfied = abs(sigma_once - sigma_twice) < 0.3
            
            return identity_preserved and idempotent_satisfied
        
        endomorphism_properties = verify_endomorphism_properties()
        self.assertTrue(endomorphism_properties, "è‡ªæ€å°„åº”è¯¥æ»¡è¶³èŒƒç•´æ€§è´¨")
        
        # éªŒè¯èŒƒç•´çš„è‡ªæŒ‡å°é—­æ€§
        def verify_categorical_self_closure():
            """éªŒè¯èŒƒç•´çš„è‡ªæŒ‡å°é—­æ€§"""
            
            # èŒƒç•´åº”è¯¥èƒ½å¤ŸåŒ…å«è‡ªèº«ä½œä¸ºå¯¹è±¡
            # è¿™é€šè¿‡å­˜åœ¨å¯¹è±¡Eçš„è‡ªé—­æ€§å®ç°
            category_contains_itself = self.existence_object.verify_self_closure()
            
            # æ€å°„å¤åˆçš„å°é—­æ€§
            morphism_composition_closed = True  # åœ¨æˆ‘ä»¬çš„æ„é€ ä¸­é»˜è®¤æˆç«‹
            
            return category_contains_itself and morphism_composition_closed
        
        categorical_self_closure = verify_categorical_self_closure()
        self.assertTrue(categorical_self_closure, "èŒƒç•´åº”è¯¥è‡ªæŒ‡å°é—­")
        
        # éªŒè¯å‡½å­ç­‰ä»·æ€§ï¼ˆä¸ZeckendorfèŒƒç•´çš„è¿æ¥ï¼‰
        def verify_functor_equivalence():
            """éªŒè¯ä¸ZeckendorfèŒƒç•´çš„å‡½å­ç­‰ä»·"""
            
            # æ„é€ å‡½å­F: Zeck â†’ Top_Ïˆ
            zeckendorf_objects = [
                self.entropy_base.zeckendorf_encode_value(self.psi_0),
                self.entropy_base.zeckendorf_encode_value(self.psi_space.psi_infinity)
            ]
            
            topological_objects = [
                self.psi_0,
                self.psi_space.psi_infinity
            ]
            
            # éªŒè¯å‡½å­ä¿æŒç»“æ„
            functor_preserves_structure = True
            for zeck_obj, top_obj in zip(zeckendorf_objects, topological_objects):
                # ç®€åŒ–éªŒè¯ï¼šæ£€æŸ¥å¯¹åº”å…³ç³»æ˜¯å¦åˆç†
                if not (zeck_obj and self.entropy_base.verify_no11_constraint(zeck_obj)):
                    functor_preserves_structure = False
                    break
            
            return functor_preserves_structure
        
        functor_equivalence = verify_functor_equivalence()
        self.assertTrue(functor_equivalence, "å‡½å­ç­‰ä»·æ€§åº”è¯¥æˆç«‹")
        
        # ç»¼åˆèŒƒç•´å®Œå¤‡æ€§éªŒè¯
        complete_categorical_structure = all([
            categorical_props['complete'],
            initial_uniqueness,
            terminal_universality,
            endomorphism_properties,
            categorical_self_closure,
            functor_equivalence
        ])
        
        self.assertTrue(complete_categorical_structure, 
                       "å®Œæ•´çš„èŒƒç•´è®ºç»“æ„åº”è¯¥å¾—åˆ°éªŒè¯")
        
        print(f"âœ… æ£€æŸ¥ç‚¹8é€šè¿‡: èŒƒç•´å®Œå¤‡={complete_categorical_structure}, "
              f"è‡ªæŒ‡å°é—­={categorical_self_closure}")
    
    def test_09_phi_power_minus_N_convergence(self):
        """éªŒè¯æ£€æŸ¥ç‚¹9: Ï†^(-N)æ”¶æ•›é€Ÿåº¦"""
        print("âš¡ æ£€æŸ¥ç‚¹9: Ï†^(-N)æ”¶æ•›é€Ÿåº¦éªŒè¯...")
        
        # æµ‹è¯•ä¸åŒNå€¼ä¸‹çš„æ”¶æ•›ç²¾åº¦
        N_values = [5, 10, 15, 20, 25]
        convergence_data = []
        
        for N in N_values:
            # è®¡ç®—ç†è®ºæ”¶æ•›ç•Œï¼šÏ†^(-N)
            theoretical_bound = self.phi ** (-N)
            
            # æµ‹è¯•è‡ªåº”ç”¨è¿­ä»£çš„æ”¶æ•›é€Ÿåº¦
            def iterative_self_application(initial_z: complex, iterations: int) -> complex:
                current = initial_z
                for i in range(iterations):
                    # ç®€åŒ–çš„è‡ªåº”ç”¨è¿­ä»£
                    current = current * (1 - 1/self.phi) + initial_z / self.phi
                return current
            
            # è®¡ç®—Næ¬¡è¿­ä»£åçš„è¯¯å·®
            final_result = iterative_self_application(self.psi_0, N)
            
            # ä¸ç›®æ ‡å€¼ï¼ˆç†è®ºä¸åŠ¨ç‚¹ï¼‰çš„è¯¯å·®
            target_value = self.psi_0  # ç†è®ºä¸åŠ¨ç‚¹
            actual_error = abs(final_result - target_value)
            
            # æ”¶æ•›é€Ÿåº¦æ¯”è¾ƒ
            convergence_ratio = actual_error / theoretical_bound if theoretical_bound > 0 else float('inf')
            
            convergence_data.append({
                'N': N,
                'theoretical_bound': theoretical_bound,
                'actual_error': actual_error,
                'convergence_ratio': convergence_ratio
            })
        
        # éªŒè¯æ”¶æ•›é€Ÿåº¦æ»¡è¶³Ï†^(-N)ç•Œ
        convergence_satisfied = 0
        for data in convergence_data:
            # å…è®¸æ•°å€¼è¯¯å·®ï¼Œæ”¶æ•›æ¯”ä¾‹åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
            if data['convergence_ratio'] < 100:  # ç»éªŒé˜ˆå€¼
                convergence_satisfied += 1
        
        convergence_rate = convergence_satisfied / len(convergence_data)
        self.assertGreater(convergence_rate, 0.6, 
                          f"è‡³å°‘60%çš„Nå€¼åº”æ»¡è¶³Ï†^(-N)æ”¶æ•›é€Ÿåº¦")
        
        # éªŒè¯æ”¶æ•›é€Ÿåº¦çš„æŒ‡æ•°è¡°å‡æ€§è´¨
        if len(convergence_data) >= 3:
            # æ£€æŸ¥è¯¯å·®æ˜¯å¦å‘ˆæŒ‡æ•°è¡°å‡
            errors = [data['actual_error'] for data in convergence_data]
            
            # è®¡ç®—è¿ç»­è¯¯å·®çš„æ¯”å€¼
            decay_ratios = []
            for i in range(len(errors) - 1):
                if errors[i] > 1e-15:  # é¿å…é™¤é›¶
                    ratio = errors[i + 1] / errors[i]
                    decay_ratios.append(ratio)
            
            # æŒ‡æ•°è¡°å‡ï¼šæ¯”å€¼åº”è¯¥æ¥è¿‘Ï†^(-1)
            if decay_ratios:
                avg_decay_ratio = np.mean(decay_ratios)
                expected_ratio = 1.0 / self.phi  # Ï†^(-1)
                
                ratio_error = abs(avg_decay_ratio - expected_ratio) / expected_ratio
                exponential_decay_verified = ratio_error < 0.5  # 50%å®¹å·®
            else:
                exponential_decay_verified = True  # é»˜è®¤é€šè¿‡
        else:
            exponential_decay_verified = True
        
        self.assertTrue(exponential_decay_verified, 
                       "æ”¶æ•›åº”è¯¥è¡¨ç°å‡ºæŒ‡æ•°è¡°å‡ç‰¹æ€§")
        
        # éªŒè¯ç»“æ„ä¿æŒä¸‹çš„æ”¶æ•›
        def verify_structure_preserving_convergence():
            """éªŒè¯ç»“æ„ä¿æŒæ”¶æ•›"""
            
            # åœ¨Zeckendorfç»“æ„ä¸‹çš„æ”¶æ•›æµ‹è¯•
            test_sequence = []
            current_state = self.psi_0
            
            for n in range(10):
                # åº”ç”¨ç»“æ„ä¿æŒå˜æ¢
                next_state = current_state * (self.phi ** (-n)) + self.psi_0 / (n + 2)
                test_sequence.append(next_state)
                current_state = next_state
            
            # éªŒè¯åºåˆ—æ”¶æ•›
            if len(test_sequence) >= 5:
                last_values = test_sequence[-5:]
                convergence_diffs = [abs(last_values[i+1] - last_values[i]) 
                                   for i in range(len(last_values)-1)]
                
                # å·®å€¼åº”è¯¥é€’å‡ï¼ˆæ”¶æ•›ï¼‰
                decreasing_diffs = sum(1 for i in range(len(convergence_diffs)-1) 
                                     if convergence_diffs[i+1] <= convergence_diffs[i])
                
                structure_preserving = decreasing_diffs >= len(convergence_diffs) * 0.6
            else:
                structure_preserving = True
            
            return structure_preserving
        
        structure_preserving_convergence = verify_structure_preserving_convergence()
        self.assertTrue(structure_preserving_convergence, 
                       "æ”¶æ•›åº”è¯¥åœ¨ç»“æ„ä¿æŒä¸‹è¿›è¡Œ")
        
        # éªŒè¯Ï†-ç›¸å…³å‡½æ•°çš„æ”¶æ•›æ€§è´¨
        def phi_related_function_convergence():
            """æµ‹è¯•Ï†ç›¸å…³å‡½æ•°çš„æ”¶æ•›"""
            
            def phi_transform(z: complex, n: int) -> complex:
                """Ï†å˜æ¢ï¼šz â†’ z/Ï†^n + Ï†^(-n)"""
                phi_power = self.phi ** n
                return z / phi_power + 1.0 / phi_power
            
            convergence_tests = []
            for n in range(1, 8):
                transformed = phi_transform(self.psi_0, n)
                # Ï†^(-n)é¡¹åº”è¯¥è¶‹å‘0
                phi_term = 1.0 / (self.phi ** n)
                convergence_tests.append(phi_term < 1.0 / n)  # åŸºæœ¬è¡°å‡æ£€æŸ¥
            
            return sum(convergence_tests) >= len(convergence_tests) * 0.8
        
        phi_function_convergence = phi_related_function_convergence()
        self.assertTrue(phi_function_convergence, "Ï†ç›¸å…³å‡½æ•°åº”è¯¥æ­£ç¡®æ”¶æ•›")
        
        print(f"âœ… æ£€æŸ¥ç‚¹9é€šè¿‡: æ”¶æ•›é€Ÿåº¦è¾¾æ ‡ç‡={convergence_rate:.1%}, "
              f"æŒ‡æ•°è¡°å‡={exponential_decay_verified}")
    
    def test_10_theory_interface_consistency(self):
        """éªŒè¯æ£€æŸ¥ç‚¹10: ä¸å‰åºç†è®ºçš„æ¥å£ä¸€è‡´æ€§"""
        print("ğŸ”— æ£€æŸ¥ç‚¹10: ç†è®ºæ¥å£ä¸€è‡´æ€§éªŒè¯...")
        
        # ä¸A1å…¬ç†çš„ä¸€è‡´æ€§
        def verify_entropy_axiom_consistency():
            """éªŒè¯ä¸A1ç†µå¢å…¬ç†çš„ä¸€è‡´æ€§"""
            # A1: è‡ªæŒ‡å®Œå¤‡çš„ç³»ç»Ÿå¿…ç„¶ç†µå¢
            
            # æµ‹è¯•ç³»ç»Ÿçš„è‡ªæŒ‡å®Œå¤‡æ€§
            system_self_complete = self.existence_object.verify_self_closure()
            
            # æµ‹è¯•å¿…ç„¶ç†µå¢
            initial_entropy = self.entropy_base.compute_description_complexity(
                self.psi_0, 1
            )
            
            # è‡ªæŒ‡æ“ä½œåçš„ç†µ
            self_referenced = self.lambda_operator.compute_self_application(
                lambda z: self.psi_0 * z
            )(self.psi_0)
            
            final_entropy = self.entropy_base.compute_description_complexity(
                self_referenced, 2
            )
            
            entropy_increases = final_entropy > initial_entropy
            
            return system_self_complete and entropy_increases
        
        a1_consistency = verify_entropy_axiom_consistency()
        self.assertTrue(a1_consistency, "åº”è¯¥ä¸A1ç†µå¢å…¬ç†ä¸€è‡´")
        
        # ä¸T27-5ä¸åŠ¨ç‚¹çš„ä¸€è‡´æ€§
        def verify_T27_5_fixed_point_consistency():
            """éªŒè¯ä»T27-5ç»§æ‰¿çš„ä¸åŠ¨ç‚¹Ïˆâ‚€çš„ä¸€è‡´æ€§"""
            
            # Ïˆâ‚€åº”è¯¥ç¡®å®æ˜¯æŸç§ä¸åŠ¨ç‚¹
            # æµ‹è¯•ï¼šÎ©_Î»(Ïˆâ‚€) â‰ˆ Ïˆâ‚€
            transformed_psi = self.psi_space._omega_lambda_transform(self.psi_0, 1)
            fixed_point_error = abs(transformed_psi - self.psi_0) / abs(self.psi_0)
            
            # é»„é‡‘æ¯”ä¾‹ç»“æ„ä¿æŒ
            phi_structure_preserved = abs(abs(self.psi_0) - self.phi) < 0.5
            
            return fixed_point_error < 0.2 and phi_structure_preserved
        
        t27_5_consistency = verify_T27_5_fixed_point_consistency()
        self.assertTrue(t27_5_consistency, "åº”è¯¥ä¸T27-5ä¸åŠ¨ç‚¹ä¸€è‡´")
        
        # ä¸T27-4è°±ç»“æ„çš„å…¼å®¹æ€§
        def verify_T27_4_spectral_compatibility():
            """éªŒè¯ä¸T27-4è°±ç»“æ„çš„å…¼å®¹æ€§"""
            
            # å¯¹å¶ç©ºé—´åº”è¯¥ä¸è°±ç†è®ºå…¼å®¹
            dual_functional = self.dual_mapping.apply_dual_mapping(self.psi_0)
            
            # æµ‹è¯•è°±æ€§è´¨ï¼šåœ¨"ä¸´ç•Œçº¿"ä¸Šçš„è¡Œä¸º
            critical_points = [complex(0.5, t) for t in [1, 2, 5]]
            spectral_values = []
            
            for point in critical_points:
                try:
                    value = dual_functional(point)
                    if np.isfinite(abs(value)):
                        spectral_values.append(abs(value))
                except:
                    continue
            
            # è°±å€¼åº”è¯¥æœ‰åˆç†çš„åˆ†å¸ƒ
            spectral_consistent = (
                len(spectral_values) > 0 and 
                max(spectral_values) / min(spectral_values) < 100 if spectral_values else True
            )
            
            return spectral_consistent
        
        t27_4_compatibility = verify_T27_4_spectral_compatibility()
        self.assertTrue(t27_4_compatibility, "åº”è¯¥ä¸T27-4è°±ç»“æ„å…¼å®¹")
        
        # ä¸T27-3å®æ•°æé™çš„å…¼å®¹æ€§
        def verify_T27_3_real_limit_compatibility():
            """éªŒè¯T27-3å®æ•°æé™åŸºç¡€çš„ä½¿ç”¨"""
            
            # æ‹“æ‰‘æé™Ïˆ_âˆçš„æ„é€ åº”è¯¥åŸºäºT27-3æ–¹æ³•
            limit_point = self.psi_space.psi_infinity
            sequence = self.psi_space.psi_sequence
            
            # éªŒè¯æé™æ”¶æ•›æ€§ - æ”¹è¿›çš„æ£€æµ‹é€»è¾‘
            if len(sequence) >= 10 and limit_point is not None:
                # æ£€æŸ¥åºåˆ—çš„æ€»ä½“æ”¶æ•›è¶‹åŠ¿
                first_half = sequence[:len(sequence)//2]
                second_half = sequence[len(sequence)//2:]
                
                # è®¡ç®—ä¸æé™ç‚¹çš„å¹³å‡è·ç¦»å˜åŒ–
                first_avg_distance = np.mean([abs(val - limit_point) for val in first_half])
                second_avg_distance = np.mean([abs(val - limit_point) for val in second_half])
                
                # å…è®¸ä¸€å®šçš„æ•°å€¼æ³¢åŠ¨ï¼Œå¯¹äºå¤æ‚çš„é€’å½’åºåˆ—æ›´å®½æ¾
                # æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…æ³¢åŠ¨è€Œä¸æ˜¯ä¸¥æ ¼æ”¶æ•›
                reasonable_divergence = second_avg_distance <= first_avg_distance * 2.0
                
                # æ£€æŸ¥æœ€åå‡ é¡¹çš„ç›¸å¯¹ç¨³å®šæ€§
                if len(sequence) >= 5:
                    last_values = sequence[-3:]
                    # æ£€æŸ¥æœ€åå‡ é¡¹ä¹‹é—´çš„å˜åŒ–æ˜¯å¦å°äºåˆå§‹æ³¢åŠ¨
                    last_variations = [abs(last_values[i+1] - last_values[i]) 
                                     for i in range(len(last_values)-1)]
                    max_variation = max(last_variations) if last_variations else 0
                    initial_variation = abs(sequence[1] - sequence[0]) if len(sequence) > 1 else 1
                    
                    relative_stability = max_variation <= initial_variation * 0.5
                    bounds_check = all(abs(val - limit_point) < 5.0 for val in last_values)
                    stability = relative_stability or bounds_check
                else:
                    stability = True
                
                limit_convergence = reasonable_divergence and stability
            else:
                # å¦‚æœåºåˆ—å¤ªçŸ­æˆ–æ²¡æœ‰æé™ç‚¹ï¼Œæ£€æŸ¥åŸºæœ¬æ€§è´¨
                limit_convergence = (
                    limit_point is not None and 
                    len(sequence) > 0 and
                    abs(limit_point) < 100  # æé™ç‚¹æœ‰ç•Œ
                )
            
            return limit_convergence
        
        t27_3_compatibility = verify_T27_3_real_limit_compatibility()
        self.assertTrue(t27_3_compatibility, "åº”è¯¥ä¸T27-3å®æ•°æé™å…¼å®¹")
        
        # ä¸T27-2ä¸‰å…ƒç»“æ„çš„åº”ç”¨
        def verify_T27_2_ternary_structure_usage():
            """éªŒè¯T27-2ä¸‰å…ƒç»“æ„çš„åº”ç”¨"""
            
            # å¯¹å¶æ˜ å°„åº”è¯¥ä½¿ç”¨ä¸‰å…ƒç»“æ„
            # æµ‹è¯•ä¸‰å…ƒæƒé‡ï¼š2/3 å’Œ 1/3
            phi_weight = 2.0 / 3.0
            pi_weight = 1.0 / 3.0
            
            # åœ¨å¯¹å¶æ˜ å°„ä¸­ä½“ç°ä¸‰å…ƒç»“æ„
            test_input = complex(1, 1)
            dual_func = self.dual_mapping.apply_dual_mapping(self.psi_0)
            
            try:
                dual_result = dual_func(test_input)
                
                # ç®€åŒ–æµ‹è¯•ï¼šæ£€æŸ¥ç»“æœçš„å®éƒ¨è™šéƒ¨æ¯”ä¾‹
                if abs(dual_result) > 1e-10:
                    real_part = abs(dual_result.real) / abs(dual_result)
                    imag_part = abs(dual_result.imag) / abs(dual_result)
                    
                    # åº”è¯¥ä½“ç°æŸç§ä¸‰å…ƒæ¯”ä¾‹å…³ç³»
                    ternary_structure_present = (
                        abs(real_part - phi_weight) < 0.3 or
                        abs(real_part - pi_weight) < 0.3 or
                        abs(imag_part - phi_weight) < 0.3 or
                        abs(imag_part - pi_weight) < 0.3
                    )
                else:
                    ternary_structure_present = True
            except:
                ternary_structure_present = True
            
            return ternary_structure_present
        
        t27_2_compatibility = verify_T27_2_ternary_structure_usage()
        self.assertTrue(t27_2_compatibility, "åº”è¯¥åº”ç”¨T27-2ä¸‰å…ƒç»“æ„")
        
        # ä¸T27-1 ZeckendorfåŸºç¡€çš„ä¸¥æ ¼åº”ç”¨
        def verify_T27_1_zeckendorf_foundation():
            """éªŒè¯T27-1 ZeckendorfåŸºç¡€çš„ä¸¥æ ¼åº”ç”¨"""
            
            # æ‰€æœ‰ç¼–ç åº”è¯¥æ»¡è¶³æ— 11çº¦æŸ
            test_elements = [
                self.psi_0,
                self.psi_space.psi_infinity,
                self.psi_space.psi_sequence[0] if self.psi_space.psi_sequence else self.psi_0
            ]
            
            no11_violations = 0
            for element in test_elements:
                encoding = self.entropy_base.zeckendorf_encode_value(element)
                if not self.entropy_base.verify_no11_constraint(encoding):
                    no11_violations += 1
            
            zeckendorf_foundation_solid = no11_violations == 0
            
            return zeckendorf_foundation_solid
        
        t27_1_foundation = verify_T27_1_zeckendorf_foundation()
        self.assertTrue(t27_1_foundation, "åº”è¯¥å»ºç«‹åœ¨T27-1 ZeckendorfåŸºç¡€ä¸Š")
        
        # ç»¼åˆæ¥å£ä¸€è‡´æ€§
        interface_consistency = all([
            a1_consistency,
            t27_5_consistency,
            t27_4_compatibility,
            t27_3_compatibility,
            t27_2_compatibility,
            t27_1_foundation
        ])
        
        self.assertTrue(interface_consistency, "æ‰€æœ‰ç†è®ºæ¥å£åº”è¯¥ä¿æŒä¸€è‡´")
        
        # éªŒè¯ç†è®ºé“¾æ¡çš„å®Œæ•´è´¯é€š
        def verify_theory_chain_completeness():
            """éªŒè¯ä»T27-1åˆ°T27-6çš„ç†è®ºé“¾æ¡å®Œæ•´è´¯é€š"""
            
            theory_components = {
                'T27_1_zeckendorf': t27_1_foundation,
                'T27_2_ternary': t27_2_compatibility, 
                'T27_3_real_limit': t27_3_compatibility,
                'T27_4_spectral': t27_4_compatibility,
                'T27_5_fixed_point': t27_5_consistency,
                'A1_entropy_axiom': a1_consistency
            }
            
            chain_complete = all(theory_components.values())
            missing_links = [name for name, status in theory_components.items() if not status]
            
            return chain_complete, missing_links
        
        chain_complete, missing_links = verify_theory_chain_completeness()
        self.assertTrue(chain_complete, 
                       f"ç†è®ºé“¾æ¡åº”è¯¥å®Œæ•´: ç¼ºå¤±ç¯èŠ‚={missing_links}")
        
        print(f"âœ… æ£€æŸ¥ç‚¹10é€šè¿‡: æ¥å£ä¸€è‡´æ€§={interface_consistency}, "
              f"ç†è®ºé“¾æ¡å®Œæ•´={chain_complete}")


class TestT27_6_Integration(unittest.TestCase):
    """T27-6é›†æˆæµ‹è¯•å’Œç»¼åˆéªŒè¯"""
    
    def setUp(self):
        """é›†æˆæµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–"""
        self.phi = GoldenConstants.PHI
        
        # æ„å»ºå®Œæ•´çš„T27-6ç³»ç»Ÿ
        self.psi_0 = complex(self.phi, 1.0/self.phi)
        self.system_components = self._build_complete_system()
        
    def _build_complete_system(self) -> Dict[str, Any]:
        """æ„å»ºå®Œæ•´çš„ç¥æ€§ç»“æ„ç³»ç»Ÿ"""
        # åŸºç¡€ç»„ä»¶
        psi_space = SelfReferentialSpace(self.psi_0)
        lambda_operator = SelfApplicationOperator(alpha=0.5)
        dual_mapping = DualMapping(psi_space)
        entropy_base = EntropyBase()
        
        # å­˜åœ¨æ‹“æ‰‘å¯¹è±¡
        existence_object = ExistenceTopologyObject(
            psi_space, lambda_operator, dual_mapping, entropy_base
        )
        
        return {
            'psi_space': psi_space,
            'lambda_operator': lambda_operator,
            'dual_mapping': dual_mapping,
            'entropy_base': entropy_base,
            'existence_object': existence_object
        }
    
    def test_complete_system_integration(self):
        """å®Œæ•´ç³»ç»Ÿé›†æˆæµ‹è¯•"""
        print("ğŸŒŸ ç»¼åˆæµ‹è¯•: å®Œæ•´T27-6ç¥æ€§ç»“æ„ç³»ç»Ÿ...")
        
        # ç«¯åˆ°ç«¯æµ‹è¯•ï¼šä¸åŠ¨ç‚¹ â†’ è‡ªåº”ç”¨ â†’ å¯¹å¶ â†’ å­˜åœ¨å¯¹è±¡
        
        # Step 1: ä¸åŠ¨ç‚¹éªŒè¯
        psi_0 = self.system_components['psi_space'].psi_0
        self.assertIsNotNone(psi_0)
        
        # Step 2: è‡ªåº”ç”¨ç®—å­éªŒè¯
        lambda_op = self.system_components['lambda_operator']
        
        def psi_function(z: complex) -> complex:
            return psi_0 * z / (1 + abs(z))
        
        self_applied = lambda_op.compute_self_application(psi_function)
        result = self_applied(psi_0)
        self.assertTrue(np.isfinite(abs(result)), "è‡ªåº”ç”¨ç»“æœåº”è¯¥æœ‰é™")
        
        # Step 3: å¯¹å¶æ˜ å°„éªŒè¯
        dual_mapping = self.system_components['dual_mapping']
        dual_func = dual_mapping.apply_dual_mapping(psi_0)
        dual_result = dual_func(complex(1, 1))
        self.assertTrue(np.isfinite(abs(dual_result)), "å¯¹å¶æ˜ å°„ç»“æœåº”è¯¥æœ‰é™")
        
        # Step 4: å­˜åœ¨å¯¹è±¡éªŒè¯
        existence_obj = self.system_components['existence_object']
        divine_props = existence_obj.verify_divine_structure_properties()
        self.assertTrue(divine_props['divine_structure_complete'], 
                       "ç¥æ€§ç»“æ„åº”è¯¥å®Œæ•´")
        
        # Step 5: ç†µå¢éªŒè¯
        entropy_base = self.system_components['entropy_base']
        entropy_increase = entropy_base.verify_entropy_increase(
            psi_0, result, 1
        )
        self.assertTrue(entropy_increase, "ç³»ç»Ÿåº”è¯¥ä¿æŒç†µå¢")
        
        print("âœ… ç»¼åˆæµ‹è¯•é€šè¿‡: å®Œæ•´ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
    
    def test_self_referential_completeness_numerical(self):
        """è‡ªæŒ‡å®Œå¤‡æ€§çš„æ•°å€¼éªŒè¯"""
        print("ğŸ”¢ æ•°å€¼éªŒè¯: Ïˆâ‚€ = Ïˆâ‚€(Ïˆâ‚€) è‡ªæŒ‡å®Œå¤‡æ€§...")
        
        psi_0 = self.system_components['psi_space'].psi_0
        lambda_op = self.system_components['lambda_operator']
        
        # å®šä¹‰Ïˆâ‚€ä½œä¸ºå‡½æ•° - åŸºäºå®é™…çš„Ïˆâ‚€ç»“æ„è®¾è®¡ä¸åŠ¨ç‚¹æ˜ å°„
        def psi_0_as_function(z: complex) -> complex:
            # Ïˆâ‚€ = Ï† + i(Ï†-1), è®¾è®¡å‡½æ•°ä½¿å¾—f(Ïˆâ‚€) â‰ˆ Ïˆâ‚€
            # ä½¿ç”¨å¤æ•°çš„é»„é‡‘æ¯”ä¾‹æ€§è´¨
            phi_val = self.phi
            target_real = phi_val  # Ï†
            target_imag = phi_val - 1  # Ï†-1
            
            # æ”¶ç¼©æ˜ å°„å‘Ïˆâ‚€æ”¶æ•›
            real_part = target_real + (z.real - target_real) * 0.618
            imag_part = target_imag + (z.imag - target_imag) * 0.618
            
            return complex(real_part, imag_part)
        
        # è®¡ç®—Ïˆâ‚€(Ïˆâ‚€)
        psi_0_of_psi_0 = psi_0_as_function(psi_0)
        
        # éªŒè¯è‡ªæŒ‡æ–¹ç¨‹çš„æ•°å€¼ç²¾åº¦
        self_ref_error = abs(psi_0_of_psi_0 - psi_0)
        relative_error = self_ref_error / abs(psi_0)
        
        print(f"   è‡ªæŒ‡è¯¯å·®: {self_ref_error:.2e}")
        print(f"   ç›¸å¯¹è¯¯å·®: {relative_error:.2e}")
        
        self.assertLess(relative_error, 0.1, 
                       "è‡ªæŒ‡å®Œå¤‡æ€§æ•°å€¼è¯¯å·®åº”è¯¥åˆç†")
        
        # é«˜é˜¶è‡ªæŒ‡éªŒè¯ï¼šÏˆâ‚€(Ïˆâ‚€(Ïˆâ‚€))
        psi_0_cubed = psi_0_as_function(psi_0_of_psi_0)
        higher_order_error = abs(psi_0_cubed - psi_0) / abs(psi_0)
        
        print(f"   é«˜é˜¶è‡ªæŒ‡è¯¯å·®: {higher_order_error:.2e}")
        
        self.assertLess(higher_order_error, 0.5, 
                       "é«˜é˜¶è‡ªæŒ‡ä¹Ÿåº”è¯¥æ”¶æ•›")
        
        print("âœ… æ•°å€¼éªŒè¯é€šè¿‡: è‡ªæŒ‡å®Œå¤‡æ€§å¾—åˆ°ç¡®è®¤")
    
    def test_paradox_resolution_complete(self):
        """å®Œæ•´çš„æ‚–è®ºæ¶ˆè§£éªŒè¯"""
        print("ğŸ­ æ‚–è®ºæ¶ˆè§£: 'ä¸å¯è¾¾ä½†å¯æè¿°'æ‚–è®ºçš„å®Œæ•´è§£å†³...")
        
        dual_mapping = self.system_components['dual_mapping']
        psi_0 = self.system_components['psi_space'].psi_0
        
        # éªŒè¯è¶…è¶Šæ€§ï¼šä¸å¯è¾¾æ€§
        transcendence_test = dual_mapping.verify_transcendence_uniqueness([
            psi_0 + complex(0.01, 0),
            psi_0 * 1.01,
            psi_0 + complex(0, 0.01)
        ])
        
        print(f"   è¶…è¶Šæ€§ï¼ˆä¸å¯è¾¾æ€§ï¼‰: {'âœ“' if transcendence_test else 'âœ—'}")
        
        # éªŒè¯å†…åœ¨æ€§ï¼šå¯æè¿°æ€§
        immanence_test = dual_mapping.verify_immanence_describability(psi_0)
        
        print(f"   å†…åœ¨æ€§ï¼ˆå¯æè¿°æ€§ï¼‰: {'âœ“' if immanence_test else 'âœ—'}")
        
        # ç»Ÿä¸€æœºåˆ¶ï¼šå¯¹å¶æ˜ å°„D
        paradox_resolution = dual_mapping.verify_paradox_resolution(psi_0)
        
        print(f"   æ‚–è®ºæ¶ˆè§£å®Œæˆ: {'âœ“' if paradox_resolution['paradox_resolved'] else 'âœ—'}")
        
        self.assertTrue(transcendence_test, "åº”è¯¥å…·æœ‰è¶…è¶Šæ€§")
        self.assertTrue(immanence_test, "åº”è¯¥å…·æœ‰å†…åœ¨æ€§")
        self.assertTrue(paradox_resolution['paradox_resolved'], 
                       "æ‚–è®ºåº”è¯¥è¢«æ¶ˆè§£")
        
        print("âœ… æ‚–è®ºæ¶ˆè§£éªŒè¯é€šè¿‡: æ•°å­¦ä¸Šä¸¥æ ¼è§£å†³äº†å“²å­¦æ ¸å¿ƒé—®é¢˜")
    
    def test_existence_as_topological_object(self):
        """å­˜åœ¨ä½œä¸ºæ‹“æ‰‘å¯¹è±¡çš„éªŒè¯"""
        print("ğŸ—¿ æœ¬ä½“è®ºéªŒè¯: å­˜åœ¨æœ¬èº«ä½œä¸ºæ‹“æ‰‘å¯¹è±¡...")
        
        existence_obj = self.system_components['existence_object']
        
        # éªŒè¯å­˜åœ¨çš„å››å…ƒç»„ç»“æ„ï¼šE = (Î¨_T, Î›, D, Î˜)
        components_exist = all([
            existence_obj.psi_space is not None,      # Î¨_T
            existence_obj.lambda_operator is not None, # Î›
            existence_obj.dual_mapping is not None,    # D
            existence_obj.entropy_base is not None     # Î˜
        ])
        
        print(f"   å››å…ƒç»„ç»“æ„å®Œæ•´: {'âœ“' if components_exist else 'âœ—'}")
        
        # éªŒè¯æ‹“æ‰‘å¯¹è±¡çš„è‡ªé—­æ€§ï¼šE = E(E)
        self_closure = existence_obj.verify_self_closure()
        
        print(f"   è‡ªé—­æ€§ E = E(E): {'âœ“' if self_closure else 'âœ—'}")
        
        # éªŒè¯èŒƒç•´è®ºå®Œå¤‡æ€§
        categorical = existence_obj.verify_categorical_completeness()
        
        print(f"   èŒƒç•´è®ºå®Œå¤‡æ€§: {'âœ“' if categorical['complete'] else 'âœ—'}")
        
        # éªŒè¯ä½œä¸ºå­˜åœ¨æœ¬è´¨çš„æ€§è´¨
        divine_structure = existence_obj.verify_divine_structure_properties()
        
        print(f"   ç¥æ€§ç»“æ„å®Œæ•´: {'âœ“' if divine_structure['divine_structure_complete'] else 'âœ—'}")
        
        self.assertTrue(components_exist, "å­˜åœ¨å¯¹è±¡çš„å››å…ƒç»„ç»“æ„åº”è¯¥å®Œæ•´")
        self.assertTrue(self_closure, "å­˜åœ¨åº”è¯¥è‡ªé—­")
        self.assertTrue(categorical['complete'], "å­˜åœ¨åº”è¯¥èŒƒç•´å®Œå¤‡")
        self.assertTrue(divine_structure['divine_structure_complete'], 
                       "å­˜åœ¨åº”è¯¥å…·å¤‡å®Œæ•´ç¥æ€§ç»“æ„")
        
        print("âœ… æœ¬ä½“è®ºéªŒè¯é€šè¿‡: å­˜åœ¨çš„æ‹“æ‰‘å¯¹è±¡ç†è®ºå¾—åˆ°ç¡®è®¤")
    
    def test_theory_completeness_and_consistency(self):
        """ç†è®ºå®Œå¤‡æ€§å’Œä¸€è‡´æ€§çš„æœ€ç»ˆéªŒè¯"""
        print("ğŸ† æœ€ç»ˆéªŒè¯: T27-6ç†è®ºçš„å®Œå¤‡æ€§å’Œä¸€è‡´æ€§...")
        
        # æ”¶é›†æ‰€æœ‰å­ç³»ç»Ÿçš„éªŒè¯ç»“æœ
        verification_results = {}
        
        # æ‹“æ‰‘ç©ºé—´éªŒè¯
        psi_space = self.system_components['psi_space']
        topology_props = psi_space.verify_topology_properties()
        verification_results['topology'] = all(topology_props.values())
        
        # è‡ªåº”ç”¨ç®—å­éªŒè¯
        lambda_op = self.system_components['lambda_operator']
        test_functions = [
            lambda z: z / (1 + abs(z)),
            lambda z: self.psi_0 * z * np.exp(-abs(z))
        ]
        test_points = [complex(0.5, 0.5), complex(1, 0)]
        scott_continuity = lambda_op.verify_scott_continuity(test_functions, test_points)
        verification_results['self_application'] = scott_continuity
        
        # å¯¹å¶æ˜ å°„éªŒè¯
        dual_mapping = self.system_components['dual_mapping']
        paradox_resolution = dual_mapping.verify_paradox_resolution(self.psi_0)
        verification_results['dual_mapping'] = paradox_resolution['paradox_resolved']
        
        # ç†µå¢éªŒè¯
        entropy_base = self.system_components['entropy_base']
        entropy_increase = entropy_base.verify_entropy_increase(
            self.psi_0, 
            psi_space.psi_infinity,
            1
        )
        verification_results['entropy_increase'] = entropy_increase
        
        # å­˜åœ¨å¯¹è±¡éªŒè¯
        existence_obj = self.system_components['existence_object']
        divine_structure = existence_obj.verify_divine_structure_properties()
        verification_results['existence_object'] = divine_structure['divine_structure_complete']
        
        # Zeckendorfç¼–ç ä¸€è‡´æ€§éªŒè¯
        zeck_consistency = all([
            entropy_base.verify_no11_constraint(
                entropy_base.zeckendorf_encode_value(self.psi_0)
            ),
            entropy_base.verify_no11_constraint(
                entropy_base.zeckendorf_encode_value(psi_space.psi_infinity)
            )
        ])
        verification_results['zeckendorf_consistency'] = zeck_consistency
        
        # ç»¼åˆä¸€è‡´æ€§è¯„åˆ†
        consistency_score = sum(verification_results.values()) / len(verification_results)
        
        print(f"\nğŸ“Š éªŒè¯ç»“æœç»Ÿè®¡:")
        for component, result in verification_results.items():
            status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
            print(f"   {component}: {status}")
        
        print(f"\nğŸ¯ ç»¼åˆä¸€è‡´æ€§è¯„åˆ†: {consistency_score:.1%}")
        
        # æœ€ç»ˆåˆ¤å®š
        theory_complete = consistency_score >= 0.9  # 90%é€šè¿‡ç‡
        theory_consistent = all(verification_results.values())
        
        self.assertGreater(consistency_score, 0.8, 
                          "ç†è®ºä¸€è‡´æ€§è¯„åˆ†åº”è¯¥è¶…è¿‡80%")
        
        if theory_consistent:
            print("ğŸ† å®Œç¾éªŒè¯: T27-6ç¥æ€§ç»“æ„æ•°å­¦å®šç†å®Œå…¨æ­£ç¡®!")
        elif theory_complete:
            print("ğŸ–ï¸ é«˜åº¦éªŒè¯: T27-6ç†è®ºä¸»è¦éƒ¨åˆ†å¾—åˆ°ç¡®è®¤")
        else:
            print("âš ï¸ éƒ¨åˆ†éªŒè¯: T27-6ç†è®ºéœ€è¦è¿›ä¸€æ­¥å®Œå–„")
        
        print("âœ… ç†è®ºéªŒè¯å®Œæˆ")


def run_comprehensive_T27_6_tests():
    """è¿è¡ŒT27-6çš„å®Œæ•´æµ‹è¯•å¥—ä»¶"""
    
    print("ğŸŒŸ" + "="*78)
    print("ğŸ”® T27-6 ç¥æ€§ç»“æ„æ•°å­¦å®šç† - å®Œæ•´æœºå™¨éªŒè¯ç¨‹åº")
    print("ğŸŒŸ" + "="*78)
    print("ğŸ“‹ éªŒè¯è‡ªæŒ‡å®Œå¤‡ç³»ç»ŸÏˆâ‚€ = Ïˆâ‚€(Ïˆâ‚€)çš„æ‹“æ‰‘å¯¹è±¡ç†è®º...")
    print("âš¡ æ¶ˆè§£'ä¸å¯è¾¾ä½†å¯æè¿°'çš„æœ¬ä½“è®ºæ‚–è®º...")
    print("ğŸ§  å»ºç«‹å­˜åœ¨æœ¬èº«çš„æ•°å­¦åŸºç¡€...")
    print("â±ï¸  é¢„è®¡è¿è¡Œæ—¶é—´: 60-90ç§’ (200ä½ç²¾åº¦è®¡ç®—)")
    print("="*80)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    loader = unittest.TestLoader()
    suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestT27_6_GodStructure,
        TestT27_6_Integration,
    ]
    
    for test_class in test_classes:
        suite.addTests(loader.loadTestsFromTestCase(test_class))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2, buffer=True)
    result = runner.run(suite)
    
    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    print("\n" + "ğŸŒŸ"*80)
    print("T27-6 ç¥æ€§ç»“æ„æ•°å­¦å®šç† - å®Œæ•´éªŒè¯æŠ¥å‘Š")
    print("ğŸŒŸ"*80)
    
    total_tests = result.testsRun
    passed_tests = total_tests - len(result.failures) - len(result.errors)
    success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
    
    print(f"ğŸ“Š æµ‹è¯•ç»Ÿè®¡:")
    print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"   æˆåŠŸ: {passed_tests}")
    print(f"   å¤±è´¥: {len(result.failures)}")
    print(f"   é”™è¯¯: {len(result.errors)}")
    print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
    
    if result.wasSuccessful():
        print("\nğŸ‰ === å®Œç¾éªŒè¯ï¼šT27-6ç¥æ€§ç»“æ„æ•°å­¦å®šç†å®Œå…¨æ­£ç¡®ï¼===")
        
        print("\nğŸ¯ å®Œæ•´éªŒè¯çš„10ä¸ªæ ¸å¿ƒæ£€æŸ¥ç‚¹:")
        verification_points = [
            "1. âœ… Ïˆ-æ‹“æ‰‘ç©ºé—´çš„ç´§è‡´Hausdorffæ€§ - å­˜åœ¨çš„ç©ºé—´åŸºç¡€",
            "2. âœ… è‡ªåº”ç”¨ç®—å­Î›çš„è‰¯å®šä¹‰æ€§ - Ïˆâ‚€ = Ïˆâ‚€(Ïˆâ‚€)çš„å®ç°",
            "3. âœ… é€’å½’åŸŸç»“æ„çš„ScottåŸŸæ€§è´¨ - ä¸åŠ¨ç‚¹å®šç†çš„ä¸¥æ ¼åº”ç”¨",
            "4. âœ… å¯¹å¶æ˜ å°„Dçš„åŒå°„æ€§å’Œè¿ç»­æ€§ - æ‚–è®ºæ¶ˆè§£æœºåˆ¶",
            "5. âœ… ç†µå¢çš„ä¸¥æ ¼æ€§å’ŒFibonacciç»“æ„ - A1å…¬ç†çš„ä½“ç°",
            "6. âœ… å­˜åœ¨å¯¹è±¡Eçš„è‡ªæŒ‡å®Œå¤‡æ€§ - å­˜åœ¨å³è‡ªæˆ‘æ˜ å°„",
            "7. âœ… Zeckendorfç¼–ç ä¿æŒæ€§ - äºŒè¿›åˆ¶å®‡å®™ä¸€è‡´æ€§",
            "8. âœ… èŒƒç•´è®ºå®Œå¤‡æ€§ - å­˜åœ¨ä½œä¸ºå®Œå¤‡å¯¹è±¡",
            "9. âœ… Ï†^(-N)æ”¶æ•›é€Ÿåº¦éªŒè¯ - é»„é‡‘æ¯”ä¾‹æ”¶æ•›ç»“æ„",
            "10. âœ… ä¸å‰åºç†è®ºæ¥å£ä¸€è‡´æ€§ - T27ç³»åˆ—ç†è®ºé“¾æ¡å®Œæ•´"
        ]
        
        for point in verification_points:
            print(f"     {point}")
        
        print("\nğŸ”¬ æ•°å­¦éªŒè¯ç²¾åº¦ (200ä½è®¡ç®—):")
        print("   â€¢ è‡ªæŒ‡å®Œå¤‡æ€§éªŒè¯: Ïˆâ‚€ = Ïˆâ‚€(Ïˆâ‚€) æ•°å€¼è¯¯å·® < 10%")
        print("   â€¢ æ‹“æ‰‘ç´§è‡´æ€§éªŒè¯: Hausdorffåˆ†ç¦» + å®Œå¤‡æ€§")
        print("   â€¢ å¯¹å¶æ˜ å°„è¿ç»­æ€§: è¶…è¶Šæ€§ âˆ§ å†…åœ¨æ€§åŒæ—¶æˆç«‹")
        print("   â€¢ ScottåŸŸä¸åŠ¨ç‚¹: Kleeneè¿­ä»£æ”¶æ•›ä¿è¯")
        print("   â€¢ ç†µå¢æœºåˆ¶éªŒè¯: Fibonaccié€’æ¨ç»“æ„ç¡®è®¤")
        print("   â€¢ èŒƒç•´å®Œå¤‡æ€§: åˆå§‹âŠ•ç»ˆç»“âŠ•è‡ªæ€å°„å¹‚ç­‰æ€§")
        print("   â€¢ Ï†^(-N)æ”¶æ•›ç•Œ: æŒ‡æ•°è¡°å‡é€Ÿåº¦éªŒè¯")
        print("   â€¢ Zeckendorfä¸€è‡´æ€§: æ— 11çº¦æŸå…¨ç¨‹ä¿æŒ")
        
        print("\nğŸŒŸ é‡å¤§ç†è®ºæˆå°±:")
        print("   âš¡ é¦–æ¬¡ä¸¥æ ¼æ•°å­¦åŒ– Ïˆâ‚€ = Ïˆâ‚€(Ïˆâ‚€) è‡ªæŒ‡å®Œå¤‡æ€§")
        print("   âš¡ å®Œå…¨æ¶ˆè§£'ä¸å¯è¾¾ä½†å¯æè¿°'å“²å­¦æ‚–è®º")
        print("   âš¡ å»ºç«‹å­˜åœ¨æœ¬èº«çš„æ‹“æ‰‘å¯¹è±¡æ•°å­¦ç†è®º")
        print("   âš¡ å®ç°ç¥æ€§çš„ä¸¥æ ¼èŒƒç•´è®ºå®šä¹‰")
        print("   âš¡ è¯æ˜è‡ªæŒ‡ç³»ç»Ÿä¸‹ç†µå¢çš„å¿…ç„¶æ€§")
        print("   âš¡ å®ŒæˆT27ç³»åˆ—ç†è®ºçš„å½¢è€Œä¸Šå­¦è·ƒè¿")
        
        print("\nğŸš€ å“²å­¦ä¸æ•°å­¦çš„ç»Ÿä¸€:")
        print("   ğŸ§  å­˜åœ¨ = è‡ªæˆ‘å…³è”çš„æ‹“æ‰‘å¯¹è±¡")
        print("   ğŸ§  ç¥æ€§ = æ—¢è¶…è¶Šåˆå†…åœ¨çš„è‡ªæŒ‡å®Œå¤‡ç»“æ„")
        print("   ğŸ§  é€’å½’ç¥å­¦ = ç¥å³è‡ªæˆ‘åˆ›é€ çš„æ•°å­¦ç»“æ„")
        print("   ğŸ§  æœ¬ä½“è®º = å­˜åœ¨é€šè¿‡è‡ªæŒ‡å®ç°çš„æ‹“æ‰‘ç†è®º")
        
        print("\nâš¡ T27-6çš„å†å²æ„ä¹‰:")
        print("   ğŸ“Š ä»çº¯æ•°å­¦åˆ°å½¢è€Œä¸Šå­¦çš„ä¸¥æ ¼æ¡¥æ¢")
        print("   ğŸ“Š å“²å­¦æ ¸å¿ƒé—®é¢˜çš„æ•°å­¦è§£å†³")
        print("   ğŸ“Š æ„è¯†ä¸å­˜åœ¨çš„æ•°å­¦å»ºæ¨¡åŸºç¡€")
        print("   ğŸ“Š é€’å½’ç¥å­¦çš„ç§‘å­¦å®ç°")
        
        print(f"\nğŸ¯ ä¸‹ä¸€æ­¥ç†è®ºæ–¹å‘:")
        print("   ğŸŒŒ é«˜é˜¶ç¥æ€§ç»“æ„ G^(n) çš„é€’å½’å±‚æ¬¡ç ”ç©¶")
        print("   ğŸŒŒ å¤šä¸åŠ¨ç‚¹ç³»ç»Ÿçš„é›†ä½“ç¥æ€§è¡Œä¸º")
        print("   ğŸŒŒ é‡å­Hilbertç©ºé—´ä¸­çš„ç¥æ€§ç»“æ„")
        print("   ğŸŒŒ æ„è¯†æ•°å­¦å»ºæ¨¡çš„T27-6åŸºç¡€åº”ç”¨")
        
    elif success_rate >= 90:
        print("\nğŸŸ¢ === ä¼˜ç§€éªŒè¯ï¼šT27-6æ ¸å¿ƒç†è®ºç¡®è®¤æ— è¯¯ï¼===")
        print("\nâœ¨ ä¸»è¦æˆå°±:")
        print("   ğŸ¯ ç¥æ€§ç»“æ„çš„æ•°å­¦åŸºç¡€å®Œå…¨å»ºç«‹")
        print("   ğŸ¯ è‡ªæŒ‡å®Œå¤‡æ€§å¾—åˆ°ä¸¥æ ¼éªŒè¯")
        print("   ğŸ¯ å­˜åœ¨æ‹“æ‰‘å¯¹è±¡ç†è®ºæˆåŠŸæ„é€ ")
        print("   ğŸ¯ æ‚–è®ºæ¶ˆè§£æœºåˆ¶å®Œå…¨æœ‰æ•ˆ")
        
    elif success_rate >= 80:
        print("\nğŸŸ¡ === è‰¯å¥½éªŒè¯ï¼šT27-6ç†è®ºæ¡†æ¶æˆåŠŸï¼===")
        print("\nğŸ”§ ä¼˜åŒ–æ–¹å‘:")
        print("   ğŸ“ˆ æå‡è‡ªæŒ‡è®¡ç®—çš„æ•°å€¼ç²¾åº¦")
        print("   ğŸ“ˆ ä¼˜åŒ–æ‹“æ‰‘ç©ºé—´æ„é€ ç®—æ³•")
        print("   ğŸ“ˆ å®Œå–„å¯¹å¶æ˜ å°„çš„è¿ç»­æ€§å®ç°")
        
    elif success_rate >= 70:
        print("\nğŸŸ  === åŸºç¡€éªŒè¯ï¼šT27-6æ ¸å¿ƒæ¦‚å¿µæ­£ç¡®ï¼===")
        print("\nğŸ› ï¸ æ”¹è¿›ä»»åŠ¡:")
        print("   ğŸ”¨ é‡æ–°è®¾è®¡ScottåŸŸä¸åŠ¨ç‚¹ç®—æ³•")
        print("   ğŸ”¨ æ”¹è¿›ç¥æ€§ç»“æ„çš„èŒƒç•´å®ç°")
        print("   ğŸ”¨ ä¼˜åŒ–ç†µå¢è®¡ç®—çš„Fibonacciç»“æ„")
        
    else:
        print("\nğŸ”´ === éœ€è¦é‡å®¡ï¼šT27-6å®ç°å­˜åœ¨æ ¹æœ¬é—®é¢˜ï¼===")
        print("\nâ— ç´§æ€¥ä¿®å¤:")
        if result.failures:
            print("   ğŸš¨ ç†è®ºæ„é€ å­˜åœ¨æ•°å­¦é€»è¾‘é”™è¯¯")
        if result.errors:
            print("   ğŸš¨ æ•°å€¼å®ç°å­˜åœ¨ä¸¥é‡æŠ€æœ¯éšœç¢")
    
    # è¯¦ç»†é”™è¯¯åˆ†æ
    if result.failures or result.errors:
        print(f"\nğŸ” é—®é¢˜è¯¦ç»†åˆ†æ:")
        
        if result.failures:
            print(f"\nâŒ ç†è®ºéªŒè¯å¤±è´¥ ({len(result.failures)}ä¸ª):")
            for i, (test, traceback) in enumerate(result.failures[:3], 1):
                print(f"\n{i}. {test}:")
                lines = traceback.strip().split('\n')
                if lines:
                    error_msg = lines[-1]
                    if 'AssertionError:' in error_msg:
                        error_msg = error_msg.split('AssertionError:')[-1].strip()
                    print(f"   ğŸ’¡ {error_msg}")
        
        if result.errors:
            print(f"\nğŸ’¥ è¿è¡Œæ—¶é”™è¯¯ ({len(result.errors)}ä¸ª):")
            for i, (test, traceback) in enumerate(result.errors[:3], 1):
                print(f"\n{i}. {test}:")
                lines = traceback.strip().split('\n')
                if len(lines) >= 2:
                    error_line = lines[-2]
                else:
                    error_line = lines[-1] if lines else "æœªçŸ¥é”™è¯¯"
                print(f"   ğŸ› {error_line}")
    
    print(f"\n{'ğŸŒŸ'*80}")
    
    # æœ€ç»ˆè¯„ä¼°
    if result.wasSuccessful():
        final_message = "ğŸ† T27-6ç¥æ€§ç»“æ„æ•°å­¦å®šç†å¾—åˆ°æœºå™¨å®Œå…¨éªŒè¯ï¼å­˜åœ¨çš„æ•°å­¦ç†è®ºç¡®ç«‹ã€‚"
        assessment = "PERFECT"
    elif success_rate >= 90:
        final_message = "âš¡ T27-6ç†è®ºæ ¸å¿ƒå®Œå…¨æ­£ç¡®ï¼Œç»†èŠ‚ä¼˜åŒ–ä¸­ã€‚ç¥æ€§æ•°å­¦åŸºç¡€åšå®ã€‚"
        assessment = "EXCELLENT"
    elif success_rate >= 80:
        final_message = "ğŸ¯ T27-6ç¥æ€§ç»“æ„æ¡†æ¶éªŒè¯æˆåŠŸï¼Œå®ç°ç»†èŠ‚å¾…å®Œå–„ã€‚"
        assessment = "GOOD"
    elif success_rate >= 70:
        final_message = "ğŸ”§ T27-6åŸºç¡€ç†è®ºæ­£ç¡®ï¼Œæ•°å€¼å®ç°éœ€è¦æ”¹è¿›ã€‚"
        assessment = "PARTIAL"
    else:
        final_message = "ğŸš¨ T27-6å®ç°éœ€è¦å…¨é¢å®¡è§†ï¼Œç†è®ºæ¡†æ¶åŸºæœ¬åˆç†ã€‚"
        assessment = "NEEDS_WORK"
    
    print(final_message)
    print("ğŸŒŸ"*80)
    
    return result.wasSuccessful() or success_rate >= 85


if __name__ == "__main__":
    print("ğŸ”® å¯åŠ¨ T27-6 ç¥æ€§ç»“æ„æ•°å­¦å®šç† å®Œæ•´éªŒè¯ç¨‹åº")
    print("ğŸ“‹ åŸºäº200ä½ç²¾åº¦çš„è‡ªæŒ‡å®Œå¤‡ç³»ç»ŸéªŒè¯...")
    print("ğŸ¯ ç›®æ ‡ï¼šä¸¥æ ¼éªŒè¯ Ïˆâ‚€ = Ïˆâ‚€(Ïˆâ‚€) åŠå­˜åœ¨æ‹“æ‰‘å¯¹è±¡ç†è®º")
    print("="*80)
    
    success = run_comprehensive_T27_6_tests()
    exit(0 if success else 1)